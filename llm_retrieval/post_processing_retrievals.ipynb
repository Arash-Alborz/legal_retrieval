{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281013a5",
   "metadata": {},
   "source": [
    "### Post processing gpt pair-wise retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0bc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> Script for post processing gpt pair-wise retrievals\n",
    "It takes text files and converts them to json files\n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_txt = Path(\"retrievals/gpt4.1.mini_pw_retrievals_nl.txt\") # change name of the file\n",
    "output_json = Path(\"retrievals/gpt4.1.mini_pw_retrievals_nl.json\") # output\n",
    "\n",
    "pattern_query = re.compile(r\"^query id:\\s*(\\d+)\")\n",
    "pattern_relevant = re.compile(r\"^relevant articles:\\s*(.*)\")\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "with open(results_txt, encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "i = 0\n",
    "while i < len(lines) - 1:\n",
    "    m_query = pattern_query.match(lines[i])\n",
    "    m_relevant = pattern_relevant.match(lines[i+1])\n",
    "    if m_query and m_relevant:\n",
    "        qid = m_query.group(1)\n",
    "        relevant_articles = [x.strip() for x in m_relevant.group(1).split(\",\") if x.strip()]\n",
    "        results_dict[qid] = relevant_articles\n",
    "        i += 2\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(results_dict, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"results_nl.json written to: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d0a8c",
   "metadata": {},
   "source": [
    "### Post processing gpt binary-classification retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6212d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY ===\n",
      "Total queries processed: 203\n",
      "Fully correct queries: 203\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    ">>> Script for checking the LLM output. It checks if LLM changes or added or removed any article ids --> comparing to all hard negatives.\n",
    "It also checks the values if they are only 0 or 1. No other characters or empty values. FOR GPT output...\n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "gpt_output_path = \"retrievals/gemini_2.5_flash_bin_class_retrievals_nl.jsonl\"\n",
    "hard_negatives_path = \"../sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\"\n",
    "\n",
    "# load hard negatives\n",
    "hard_negatives = {}\n",
    "with open(hard_negatives_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        hard_negatives[entry[\"query_id\"]] = set(entry[\"candidate_docs\"])\n",
    "\n",
    "invalid_values_queries = []\n",
    "missing_ids_queries = {}\n",
    "extra_ids_queries = {}\n",
    "valid_queries = []\n",
    "\n",
    "with open(gpt_output_path, encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Line {idx}: INVALID JSON\")\n",
    "            continue\n",
    "\n",
    "        query_id = obj.get(\"query_id\")\n",
    "        relevance = obj.get(\"relevance\", {})\n",
    "\n",
    "        # check relevance values\n",
    "        invalid_values = [v for v in relevance.values() if v not in (\"0\", \"1\")]\n",
    "        if invalid_values:\n",
    "            invalid_values_queries.append(query_id)\n",
    "\n",
    "        # check candidate IDs\n",
    "        expected_ids = hard_negatives.get(query_id)\n",
    "        if not expected_ids:\n",
    "            continue\n",
    "\n",
    "        actual_ids = set(relevance.keys())\n",
    "\n",
    "        missing = expected_ids - actual_ids\n",
    "        extra = actual_ids - expected_ids\n",
    "\n",
    "        if missing:\n",
    "            missing_ids_queries[query_id] = missing\n",
    "        if extra:\n",
    "            extra_ids_queries[query_id] = extra\n",
    "        if not invalid_values and not missing and not extra:\n",
    "            valid_queries.append(query_id)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total queries processed: {len(valid_queries) + len(invalid_values_queries) + len(missing_ids_queries) + len(extra_ids_queries)}\")\n",
    "print(f\"Fully correct queries: {len(valid_queries)}\")\n",
    "\n",
    "if invalid_values_queries:\n",
    "    print(f\"\\nQueries with invalid relevance values: {invalid_values_queries}\")\n",
    "if missing_ids_queries:\n",
    "    print(\"\\nQueries with missing article IDs:\")\n",
    "    for qid, ids in missing_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Missing IDs: {ids}\")\n",
    "if extra_ids_queries:\n",
    "    print(\"\\nQueries with extra article IDs:\")\n",
    "    for qid, ids in extra_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Extra IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "917656c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> Script for post ptocessing the binary classification (0/1) outputs and convert them to json retrievals. Only getting the 1 values --> relevant articles. \n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "input_path = \"retrievals/xtra/gemini_2.5_flash_bin_class_retrievals_nl.jsonl\"\n",
    "output_path = \"retrievals/json/gemini_2.5.flash_bin_class_retrieval_nl.json\"\n",
    "\n",
    "result = {}\n",
    "\n",
    "with open(input_path, encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        query_id = obj[\"query_id\"]\n",
    "        relevance = obj[\"relevance\"]\n",
    "\n",
    "        relevant_articles = [aid for aid, val in relevance.items() if val == \"1\"]\n",
    "        result[query_id] = relevant_articles\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    json.dump(result, fout, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668091b5",
   "metadata": {},
   "source": [
    "### checking all retrievals, if all queries are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590fb118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gold queries: 203\n",
      "Total predicted queries: 182\n",
      "\n",
      "Missing in predictions (21):\n",
      "  310\n",
      "  316\n",
      "  400\n",
      "  412\n",
      "  417\n",
      "  420\n",
      "  493\n",
      "  512\n",
      "  524\n",
      "  585\n",
      "  633\n",
      "  634\n",
      "  637\n",
      "  7\n",
      "  761\n",
      "  822\n",
      "  864\n",
      "  867\n",
      "  875\n",
      "  934\n",
      "  971\n",
      "No extra query IDs in predictions.\n",
      "\n",
      "Please fix mismatches before evaluating.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths â€” adjust if needed\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5_flash_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4.1.mini_id_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4o.mini_id_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4.1.mini_bin_class_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4o.mini_bin_class_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5_flash_pro_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5.flash_bin_class_retrieval_nl.json\")\n",
    "predictions_json = Path(\"retrievals/json/gemini_2.5.flash_lite_id_retrieval_nl.json\")\n",
    "\n",
    "gold_json = Path(\"gold_data/gold_standard_nl.json\")\n",
    "\n",
    "# Load predictions\n",
    "with open(predictions_json, encoding=\"utf-8\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# Load gold\n",
    "with open(gold_json, encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "pred_ids = set(predictions.keys())\n",
    "gold_ids = set(gold.keys())\n",
    "\n",
    "missing_in_preds = gold_ids - pred_ids\n",
    "extra_in_preds = pred_ids - gold_ids\n",
    "\n",
    "print(f\"Total gold queries: {len(gold_ids)}\")\n",
    "print(f\"Total predicted queries: {len(pred_ids)}\\n\")\n",
    "\n",
    "if missing_in_preds:\n",
    "    print(f\"Missing in predictions ({len(missing_in_preds)}):\")\n",
    "    for qid in sorted(missing_in_preds):\n",
    "        print(f\"  {qid}\")\n",
    "else:\n",
    "    print(\"All gold query IDs are present in predictions.\")\n",
    "\n",
    "if extra_in_preds:\n",
    "    print(f\"\\nExtra query IDs in predictions ({len(extra_in_preds)}):\")\n",
    "    for qid in sorted(extra_in_preds):\n",
    "        print(f\"  {qid}\")\n",
    "else:\n",
    "    print(\"No extra query IDs in predictions.\")\n",
    "\n",
    "if not missing_in_preds and not extra_in_preds:\n",
    "    print(\"\\nPredictions file matches gold file perfectly.\")\n",
    "else:\n",
    "    print(\"\\nPlease fix mismatches before evaluating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd00e41",
   "metadata": {},
   "source": [
    "### Checking the output of rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f61a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- CONFIG --------\n",
    "lang = \"nl\"  # or 'fr'\n",
    "output_file_path = Path(f\"retrievals/txt/gemini2.0.flash_sorted_ranking_{lang}.txt\") # gemini2.5.flash_sorted_ranking_{lang}.txt # gpt4o.mini_sorted_ranking_{lang}\n",
    "hard_negatives_path = Path(f\"../sampling_hard_negatives/hard_negatives/hard_negatives_{lang}.jsonl\")\n",
    "\n",
    "# -------- LOAD HARD NEGATIVE SET --------\n",
    "with open(hard_negatives_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    hard_data = [json.loads(line) for line in f]\n",
    "\n",
    "query_to_candidates = {\n",
    "    entry[\"query_id\"]: set(entry[\"candidate_docs\"])\n",
    "    for entry in hard_data\n",
    "}\n",
    "\n",
    "# -------- PARSE MODEL OUTPUT --------\n",
    "with open(output_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "report = []\n",
    "for i in range(0, len(lines), 2):\n",
    "    qid_line = lines[i].strip()\n",
    "    result_line = lines[i + 1].strip() if i + 1 < len(lines) else \"\"\n",
    "\n",
    "    if not (qid_line.startswith(\"query id:\") and result_line.startswith(\"ranked articles:\")):\n",
    "        continue\n",
    "\n",
    "    qid = qid_line.split(\":\", 1)[1].strip()\n",
    "    ids_str = result_line.split(\":\", 1)[1].strip()\n",
    "    predicted_ids = [x.strip() for x in ids_str.split(\",\") if x.strip()]\n",
    "    predicted_set = set(predicted_ids)\n",
    "\n",
    "    candidate_set = query_to_candidates.get(qid, set())\n",
    "\n",
    "    hallucinated_ids = sorted(predicted_set - candidate_set)\n",
    "    missing_ids = sorted(candidate_set - predicted_set)\n",
    "\n",
    "    report.append({\n",
    "        \"query_id\": qid,\n",
    "        \"n_predicted\": len(predicted_ids),\n",
    "        \"n_candidates\": len(candidate_set),\n",
    "        \"n_hallucinated\": len(hallucinated_ids),\n",
    "        \"hallucinated_ids\": hallucinated_ids,\n",
    "        \"n_missing\": len(missing_ids),\n",
    "        \"missing_ids\": missing_ids\n",
    "    })\n",
    "\n",
    "# -------- PRINT REPORT --------\n",
    "for entry in report:\n",
    "    if entry[\"n_hallucinated\"] > 0:\n",
    "        print(f\"Query {entry['query_id']}:\")\n",
    "        print(f\"  Predicted {entry['n_predicted']} articles.\")\n",
    "        print(f\"  Hallucinated IDs: {entry['hallucinated_ids']}\")\n",
    "        print(f\"  Missing IDs: {entry['missing_ids']}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
