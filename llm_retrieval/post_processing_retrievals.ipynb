{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281013a5",
   "metadata": {},
   "source": [
    "### Post processing gpt pair-wise retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0bc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> Script for post processing gpt pair-wise retrievals\n",
    "It takes text files and converts them to json files\n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_txt = Path(\"retrievals/gpt4.1.mini_pw_retrievals_nl.txt\") # change name of the file\n",
    "output_json = Path(\"retrievals/gpt4.1.mini_pw_retrievals_nl.json\") # output\n",
    "\n",
    "pattern_query = re.compile(r\"^query id:\\s*(\\d+)\")\n",
    "pattern_relevant = re.compile(r\"^relevant articles:\\s*(.*)\")\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "with open(results_txt, encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "i = 0\n",
    "while i < len(lines) - 1:\n",
    "    m_query = pattern_query.match(lines[i])\n",
    "    m_relevant = pattern_relevant.match(lines[i+1])\n",
    "    if m_query and m_relevant:\n",
    "        qid = m_query.group(1)\n",
    "        relevant_articles = [x.strip() for x in m_relevant.group(1).split(\",\") if x.strip()]\n",
    "        results_dict[qid] = relevant_articles\n",
    "        i += 2\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(results_dict, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"results_nl.json written to: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d0a8c",
   "metadata": {},
   "source": [
    "### Post processing gpt binary-classification retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY ===\n",
      "Total queries processed: 0\n",
      "Fully correct queries: 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    ">>> Script for checking the LLM output. It checks if LLM changes or added or removed any article ids --> comparing to all hard negatives.\n",
    "It also checks the values if they are only 0 or 1. No other characters or empty values. FOR GPT output...\n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "gpt_output_path = \"retrievals/gemini_2.5_pro_task2_bin_relevance_nl.jsonl\"\n",
    "hard_negatives_path = \"../sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\"\n",
    "\n",
    "# load hard negatives\n",
    "hard_negatives = {}\n",
    "with open(hard_negatives_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        hard_negatives[entry[\"query_id\"]] = set(entry[\"candidate_docs\"])\n",
    "\n",
    "invalid_values_queries = []\n",
    "missing_ids_queries = {}\n",
    "extra_ids_queries = {}\n",
    "valid_queries = []\n",
    "\n",
    "with open(gpt_output_path, encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Line {idx}: INVALID JSON\")\n",
    "            continue\n",
    "\n",
    "        query_id = obj.get(\"query_id\")\n",
    "        relevance = obj.get(\"relevance\", {})\n",
    "\n",
    "        # check relevance values\n",
    "        invalid_values = [v for v in relevance.values() if v not in (\"0\", \"1\")]\n",
    "        if invalid_values:\n",
    "            invalid_values_queries.append(query_id)\n",
    "\n",
    "        # check candidate IDs\n",
    "        expected_ids = hard_negatives.get(query_id)\n",
    "        if not expected_ids:\n",
    "            continue\n",
    "\n",
    "        actual_ids = set(relevance.keys())\n",
    "\n",
    "        missing = expected_ids - actual_ids\n",
    "        extra = actual_ids - expected_ids\n",
    "\n",
    "        if missing:\n",
    "            missing_ids_queries[query_id] = missing\n",
    "        if extra:\n",
    "            extra_ids_queries[query_id] = extra\n",
    "        if not invalid_values and not missing and not extra:\n",
    "            valid_queries.append(query_id)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total queries processed: {len(valid_queries) + len(invalid_values_queries) + len(missing_ids_queries) + len(extra_ids_queries)}\")\n",
    "print(f\"Fully correct queries: {len(valid_queries)}\")\n",
    "\n",
    "if invalid_values_queries:\n",
    "    print(f\"\\nQueries with invalid relevance values: {invalid_values_queries}\")\n",
    "if missing_ids_queries:\n",
    "    print(\"\\nQueries with missing article IDs:\")\n",
    "    for qid, ids in missing_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Missing IDs: {ids}\")\n",
    "if extra_ids_queries:\n",
    "    print(\"\\nQueries with extra article IDs:\")\n",
    "    for qid, ids in extra_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Extra IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cedd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY ===\n",
      "Total queries processed: 5\n",
      "Fully correct queries: 5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "gemini_output_path = \"retrievals/1.jsonl\"\n",
    "hard_negatives_path = \"../sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\"\n",
    "max_queries_to_check = 150  # Set to None to process all\n",
    "\n",
    "# === Load hard negatives ===\n",
    "hard_negatives = {}\n",
    "with open(hard_negatives_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        qid = str(entry[\"query_id\"])\n",
    "        hard_negatives[qid] = set(entry[\"candidate_docs\"])\n",
    "\n",
    "# === Load Gemini output from JSONL ===\n",
    "gemini_output = {}\n",
    "with open(gemini_output_path, encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Line {idx}: INVALID JSON\")\n",
    "            continue\n",
    "\n",
    "        for query_id, relevance in obj.items():\n",
    "            gemini_output[query_id] = relevance\n",
    "\n",
    "# === Validation ===\n",
    "invalid_values_queries = []\n",
    "missing_ids_queries = {}\n",
    "extra_ids_queries = {}\n",
    "valid_queries = []\n",
    "\n",
    "query_ids = list(gemini_output.keys())\n",
    "if max_queries_to_check is not None:\n",
    "    query_ids = query_ids[:max_queries_to_check]\n",
    "\n",
    "for query_id in query_ids:\n",
    "    relevance = gemini_output[query_id]\n",
    "\n",
    "    # Check values\n",
    "    invalid = [v for v in relevance.values() if v not in (\"0\", \"1\")]\n",
    "    if invalid:\n",
    "        invalid_values_queries.append(query_id)\n",
    "\n",
    "    expected_ids = hard_negatives.get(query_id)\n",
    "    if not expected_ids:\n",
    "        continue\n",
    "\n",
    "    actual_ids = set(relevance.keys())\n",
    "    missing = expected_ids - actual_ids\n",
    "    extra = actual_ids - expected_ids\n",
    "\n",
    "    if missing:\n",
    "        missing_ids_queries[query_id] = missing\n",
    "    if extra:\n",
    "        extra_ids_queries[query_id] = extra\n",
    "\n",
    "    if not invalid and not missing and not extra:\n",
    "        valid_queries.append(query_id)\n",
    "\n",
    "# === Report ===\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "total = len(valid_queries) + len(invalid_values_queries) + len(missing_ids_queries) + len(extra_ids_queries)\n",
    "print(f\"Total queries processed: {total}\")\n",
    "print(f\"Fully correct queries: {len(valid_queries)}\")\n",
    "\n",
    "if invalid_values_queries:\n",
    "    print(f\"\\nQueries with invalid relevance values: {invalid_values_queries}\")\n",
    "if missing_ids_queries:\n",
    "    print(\"\\nQueries with missing article IDs:\")\n",
    "    for qid, ids in missing_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Missing IDs: {sorted(ids)}\")\n",
    "if extra_ids_queries:\n",
    "    print(\"\\nQueries with extra article IDs:\")\n",
    "    for qid, ids in extra_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Extra IDs: {sorted(ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "917656c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> Script for post ptocessing the binary classification (0/1) outputs and convert them to json retrievals. Only getting the 1 values --> relevant articles. \n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "input_path = \"retrievals/xtra/gpt4.1.mini_bin_class_retrievals_nl.jsonl\"\n",
    "output_path = \"retrievals/json/gpt4.1.mini_bin_class_retrievals_nl.json\"\n",
    "\n",
    "result = {}\n",
    "\n",
    "with open(input_path, encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        query_id = obj[\"query_id\"]\n",
    "        relevance = obj[\"relevance\"]\n",
    "\n",
    "        relevant_articles = [aid for aid, val in relevance.items() if val == \"1\"]\n",
    "        result[query_id] = relevant_articles\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    json.dump(result, fout, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668091b5",
   "metadata": {},
   "source": [
    "### checking all retrievals, if all queries are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "590fb118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gold queries: 203\n",
      "Total predicted queries: 203\n",
      "\n",
      "All gold query IDs are present in predictions.\n",
      "No extra query IDs in predictions.\n",
      "\n",
      "Predictions file matches gold file perfectly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths â€” adjust if needed\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5_flash_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4.1.mini_id_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4o.mini_id_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4.1.mini_bin_class_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4o.mini_bin_class_retrievals_nl.json\")\n",
    "predictions_json = Path(\"retrievals/json/gemini_2.5_flash_pro_id_retrieval_nl.json\")\n",
    "gold_json = Path(\"gold_data/gold_standard_nl.json\")\n",
    "\n",
    "# Load predictions\n",
    "with open(predictions_json, encoding=\"utf-8\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# Load gold\n",
    "with open(gold_json, encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "pred_ids = set(predictions.keys())\n",
    "gold_ids = set(gold.keys())\n",
    "\n",
    "missing_in_preds = gold_ids - pred_ids\n",
    "extra_in_preds = pred_ids - gold_ids\n",
    "\n",
    "print(f\"Total gold queries: {len(gold_ids)}\")\n",
    "print(f\"Total predicted queries: {len(pred_ids)}\\n\")\n",
    "\n",
    "if missing_in_preds:\n",
    "    print(f\"Missing in predictions ({len(missing_in_preds)}):\")\n",
    "    for qid in sorted(missing_in_preds):\n",
    "        print(f\"  {qid}\")\n",
    "else:\n",
    "    print(\"All gold query IDs are present in predictions.\")\n",
    "\n",
    "if extra_in_preds:\n",
    "    print(f\"\\nExtra query IDs in predictions ({len(extra_in_preds)}):\")\n",
    "    for qid in sorted(extra_in_preds):\n",
    "        print(f\"  {qid}\")\n",
    "else:\n",
    "    print(\"No extra query IDs in predictions.\")\n",
    "\n",
    "if not missing_in_preds and not extra_in_preds:\n",
    "    print(\"\\nPredictions file matches gold file perfectly.\")\n",
    "else:\n",
    "    print(\"\\nPlease fix mismatches before evaluating.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
