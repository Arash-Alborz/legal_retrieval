{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281013a5",
   "metadata": {},
   "source": [
    "### Post processing text output of ID retrievals (gpt, llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3e0bc7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_nl.json written to: retrievals/json/llama4.scout_id_retrieval_nl.json\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    ">>> Script for post processing gpt and llama ID retrievals...\n",
    "It takes text files and converts them to json files for evaluation.\n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_txt = Path(\"retrievals/txt/llama4.scout_id_retrieval_nl.txt\") # change name of the file [llama3.3.70b_id_retrieval_{lang}.txt], [gpt4.1.mini_pw_retrievals_nl.txt], [qwen3-235B_id_retrieval_nl.txt]\n",
    "output_json = Path(\"retrievals/json/llama4.scout_id_retrieval_nl.json\") # output\n",
    "\n",
    "pattern_query = re.compile(r\"^query id:\\s*(\\d+)\")\n",
    "pattern_relevant = re.compile(r\"^relevant articles:\\s*(.*)\")\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "with open(results_txt, encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "i = 0\n",
    "while i < len(lines) - 1:\n",
    "    m_query = pattern_query.match(lines[i])\n",
    "    m_relevant = pattern_relevant.match(lines[i+1])\n",
    "    if m_query and m_relevant:\n",
    "        qid = m_query.group(1)\n",
    "        relevant_articles = [x.strip() for x in m_relevant.group(1).split(\",\") if x.strip()]\n",
    "        results_dict[qid] = relevant_articles\n",
    "        i += 2\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(results_dict, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"results_nl.json written to: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d0a8c",
   "metadata": {},
   "source": [
    "### Post processing gpt binary-classification retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3a275681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Rewritten to proper JSONL with string-normalized relevance. Total entries: 204\n",
      "ðŸ“„ Saved to: retrievals/llama4.scout_bin_class_retrieval_nl_fixed.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"retrievals/llama4.scout_bin_full_class_retrievals_nl.jsonl\"\n",
    "output_path = \"retrievals/llama4.scout_bin_class_retrieval_nl_fixed.jsonl\"\n",
    "\n",
    "buffer = \"\"\n",
    "results = []\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        buffer += line\n",
    "        if line.strip().endswith(\"}\"):  # crude heuristic: object may be complete\n",
    "            try:\n",
    "                obj = json.loads(buffer)\n",
    "\n",
    "                # --- Normalize relevance field ---\n",
    "                if \"relevance\" in obj and isinstance(obj[\"relevance\"], dict):\n",
    "                    normalized_relevance = {}\n",
    "                    for k, v in obj[\"relevance\"].items():\n",
    "                        # Convert both keys and values to strings\n",
    "                        str_k = str(k)\n",
    "                        str_v = str(v) if v in [0, 1, \"0\", \"1\"] else \"?\"\n",
    "                        normalized_relevance[str_k] = str_v\n",
    "                    obj[\"relevance\"] = normalized_relevance\n",
    "\n",
    "                results.append(obj)\n",
    "                buffer = \"\"\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # wait for more lines\n",
    "\n",
    "# Write clean JSONL\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for obj in results:\n",
    "        json_line = json.dumps(obj, ensure_ascii=False)\n",
    "        out_f.write(json_line + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Rewritten to proper JSONL with string-normalized relevance. Total entries: {len(results)}\")\n",
    "print(f\"ðŸ“„ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6212d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY ===\n",
      "Total queries processed: 204\n",
      "Fully correct queries: 204\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    ">>> Script for checking the LLM output. It checks if LLM changes or added or removed any article ids --> comparing to all hard negatives.\n",
    "It also checks the values if they are only 0 or 1. No other characters or empty values. FOR GPT output...\n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "gpt_output_path = \"retrievals/llama4.scout_bin_class_retrieval_nl_fixed.jsonl\"\n",
    "hard_negatives_path = \"../sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\"\n",
    "\n",
    "# load hard negatives\n",
    "hard_negatives = {}\n",
    "with open(hard_negatives_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        hard_negatives[entry[\"query_id\"]] = set(entry[\"candidate_docs\"])\n",
    "\n",
    "invalid_values_queries = []\n",
    "missing_ids_queries = {}\n",
    "extra_ids_queries = {}\n",
    "valid_queries = []\n",
    "\n",
    "with open(gpt_output_path, encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Line {idx}: INVALID JSON\")\n",
    "            continue\n",
    "\n",
    "        query_id = obj.get(\"query_id\")\n",
    "        relevance = obj.get(\"relevance\", {})\n",
    "\n",
    "        # check relevance values\n",
    "        invalid_values = [v for v in relevance.values() if v not in (\"0\", \"1\")]\n",
    "        if invalid_values:\n",
    "            invalid_values_queries.append(query_id)\n",
    "\n",
    "        # check candidate IDs\n",
    "        expected_ids = hard_negatives.get(query_id)\n",
    "        if not expected_ids:\n",
    "            continue\n",
    "\n",
    "        actual_ids = set(relevance.keys())\n",
    "\n",
    "        missing = expected_ids - actual_ids\n",
    "        extra = actual_ids - expected_ids\n",
    "\n",
    "        if missing:\n",
    "            missing_ids_queries[query_id] = missing\n",
    "        if extra:\n",
    "            extra_ids_queries[query_id] = extra\n",
    "        if not invalid_values and not missing and not extra:\n",
    "            valid_queries.append(query_id)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total queries processed: {len(valid_queries) + len(invalid_values_queries) + len(missing_ids_queries) + len(extra_ids_queries)}\")\n",
    "print(f\"Fully correct queries: {len(valid_queries)}\")\n",
    "\n",
    "if invalid_values_queries:\n",
    "    print(f\"\\nQueries with invalid relevance values: {invalid_values_queries}\")\n",
    "if missing_ids_queries:\n",
    "    print(\"\\nQueries with missing article IDs:\")\n",
    "    for qid, ids in missing_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Missing IDs: {ids}\")\n",
    "if extra_ids_queries:\n",
    "    print(\"\\nQueries with extra article IDs:\")\n",
    "    for qid, ids in extra_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Extra IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "917656c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> Script for post ptocessing the binary classification (0/1) outputs and convert them to json retrievals. Only getting the 1 values --> relevant articles. \n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "input_path = \"retrievals/llama4.scout_bin_class_retrieval_nl_fixed.jsonl\"\n",
    "output_path = \"retrievals/json/llama4.scout_bin_class_retrieval_nl.json\"\n",
    "\n",
    "result = {}\n",
    "\n",
    "with open(input_path, encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        query_id = obj[\"query_id\"]\n",
    "        relevance = obj[\"relevance\"]\n",
    "\n",
    "        relevant_articles = [aid for aid, val in relevance.items() if val == \"1\"]\n",
    "        result[query_id] = relevant_articles\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    json.dump(result, fout, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668091b5",
   "metadata": {},
   "source": [
    "### checking all retrievals, if all queries are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "590fb118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gold queries: 203\n",
      "Total predicted queries: 203\n",
      "\n",
      "All gold query IDs are present in predictions.\n",
      "No extra query IDs in predictions.\n",
      "\n",
      "Predictions file matches gold file perfectly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths â€” adjust if needed\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5_flash_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4.1.mini_id_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4o.mini_id_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4.1.mini_bin_class_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4o.mini_bin_class_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5_flash_pro_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5.flash_bin_class_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5.pro_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/llama3.3.70b_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/llama3.3.70b_bin_class_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/qwen3.235b_bin_class_retrieval_nl.json\")\n",
    "predictions_json = Path(\"retrievals/json/llama4.scout_bin_class_retrieval_nl.json\")\n",
    "\n",
    "gold_json = Path(\"gold_data/gold_standard_nl.json\")\n",
    "\n",
    "# Load predictions\n",
    "with open(predictions_json, encoding=\"utf-8\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# Load gold\n",
    "with open(gold_json, encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "pred_ids = set(predictions.keys())\n",
    "gold_ids = set(gold.keys())\n",
    "\n",
    "missing_in_preds = gold_ids - pred_ids\n",
    "extra_in_preds = pred_ids - gold_ids\n",
    "\n",
    "print(f\"Total gold queries: {len(gold_ids)}\")\n",
    "print(f\"Total predicted queries: {len(pred_ids)}\\n\")\n",
    "\n",
    "if missing_in_preds:\n",
    "    print(f\"Missing in predictions ({len(missing_in_preds)}):\")\n",
    "    for qid in sorted(missing_in_preds):\n",
    "        print(f\"  {qid}\")\n",
    "else:\n",
    "    print(\"All gold query IDs are present in predictions.\")\n",
    "\n",
    "if extra_in_preds:\n",
    "    print(f\"\\nExtra query IDs in predictions ({len(extra_in_preds)}):\")\n",
    "    for qid in sorted(extra_in_preds):\n",
    "        print(f\"  {qid}\")\n",
    "else:\n",
    "    print(\"No extra query IDs in predictions.\")\n",
    "\n",
    "if not missing_in_preds and not extra_in_preds:\n",
    "    print(\"\\nPredictions file matches gold file perfectly.\")\n",
    "else:\n",
    "    print(\"\\nPlease fix mismatches before evaluating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd00e41",
   "metadata": {},
   "source": [
    "## Checking the Output of Sorted Ranked Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6f61a186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to regex messy-text parser...\n",
      "Parsed 203 queries from rankings/gpt4o-mini_sorted_ranking_nl.txt\n",
      "\n",
      "===== OVERALL REPORT =====\n",
      "Total queries in hard negatives: 203\n",
      "Total queries parsed: 203\n",
      "Queries with any issues: 202 (99.51%)\n",
      "Missing article IDs: 10371 (51.09%)\n",
      "Extra (hallucinated) article IDs: 6715 (33.08%)\n",
      "Duplicate article IDs: 2606 (12.84%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# -------- CONFIG --------\n",
    "output_file_path = Path(\"rankings/gpt4o-mini_sorted_ranking_nl.txt\")\n",
    "hard_negatives_path = Path(\"../sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\")\n",
    "\n",
    "# -------- LOAD HARD NEGATIVE SET --------\n",
    "with open(hard_negatives_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    hard_data = [json.loads(line) for line in f]\n",
    "\n",
    "query_to_candidates = {\n",
    "    str(entry[\"query_id\"]).strip(): set(str(doc) for doc in entry[\"candidate_docs\"])\n",
    "    for entry in hard_data\n",
    "}\n",
    "\n",
    "# -------- TRY JSONL / JSON ARRAY PARSE --------\n",
    "def try_json_parse(path):\n",
    "    def get_field(obj, *keys):\n",
    "        \"\"\"Return the first available key value.\"\"\"\n",
    "        for k in keys:\n",
    "            if k in obj:\n",
    "                return obj[k]\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Try JSON array\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]\n",
    "        return {\n",
    "            str(get_field(obj, \"query_id\", \"query id\")).strip():\n",
    "                re.findall(r\"\\d+\", str(get_field(obj, \"ranked_articles\", \"ranked articles\")))\n",
    "            for obj in data\n",
    "        }\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Try JSONL\n",
    "    parsed = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                qid = get_field(obj, \"query_id\", \"query id\")\n",
    "                ranks = get_field(obj, \"ranked_articles\", \"ranked articles\")\n",
    "                if qid is not None:\n",
    "                    parsed[str(qid).strip()] = re.findall(r\"\\d+\", str(ranks))\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "    return parsed if parsed else None\n",
    "\n",
    "queries = try_json_parse(output_file_path)\n",
    "\n",
    "# -------- FALLBACK: MESSY GPT-STYLE TXT --------\n",
    "if queries is None:\n",
    "    print(\"Falling back to regex messy-text parser...\")\n",
    "    queries = {}\n",
    "    current_qid = None\n",
    "    current_ids = []\n",
    "\n",
    "    with open(output_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Detect query id (underscore or space)\n",
    "            qid_match = re.search(r'\"?query[_ ]id\"?\\s*:\\s*\"?(\\d+)\"?', line, re.IGNORECASE)\n",
    "            if qid_match:\n",
    "                if current_qid is not None:\n",
    "                    queries[current_qid] = current_ids\n",
    "                current_qid = qid_match.group(1)\n",
    "                current_ids = []\n",
    "                continue\n",
    "\n",
    "            # Detect ranked articles line\n",
    "            if \"ranked articles\" in line.lower():\n",
    "                ids_in_line = re.findall(r\"\\d+\", line)\n",
    "                current_ids.extend(ids_in_line)\n",
    "                continue\n",
    "\n",
    "            # Detect standalone article IDs\n",
    "            ids_in_line = re.findall(r\"\\d+\", line)\n",
    "            if ids_in_line:\n",
    "                current_ids.extend(ids_in_line)\n",
    "\n",
    "        # Save last query\n",
    "        if current_qid is not None:\n",
    "            queries[current_qid] = current_ids\n",
    "\n",
    "print(f\"Parsed {len(queries)} queries from {output_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# -------- OVERALL VALIDATION --------\n",
    "total_missing = 0\n",
    "total_extra = 0\n",
    "total_duplicates = 0\n",
    "queries_with_issues = 0\n",
    "\n",
    "for qid, candidates in query_to_candidates.items():\n",
    "    predicted = queries.get(qid, [])\n",
    "    pred_set = set(predicted)\n",
    "\n",
    "    missing_ids = candidates - pred_set\n",
    "    extra_ids = pred_set - candidates\n",
    "    dupes = [doc for doc, count in Counter(predicted).items() if count > 1]\n",
    "\n",
    "    total_missing += len(missing_ids)\n",
    "    total_extra += len(extra_ids)\n",
    "    total_duplicates += len(dupes)\n",
    "\n",
    "    if len(predicted) != 100 or missing_ids or extra_ids or dupes:\n",
    "        queries_with_issues += 1\n",
    "\n",
    "# -------- REPORT --------\n",
    "total_expected_articles = len(query_to_candidates) * 100\n",
    "\n",
    "print(\"\\n===== OVERALL REPORT =====\")\n",
    "print(f\"Total queries in hard negatives: {len(query_to_candidates)}\")\n",
    "print(f\"Total queries parsed: {len(queries)}\")\n",
    "print(f\"Queries with any issues: {queries_with_issues} ({queries_with_issues / len(query_to_candidates) * 100:.2f}%)\")\n",
    "print(f\"Missing article IDs: {total_missing} ({total_missing / total_expected_articles * 100:.2f}%)\")\n",
    "print(f\"Extra (hallucinated) article IDs: {total_extra} ({total_extra / total_expected_articles * 100:.2f}%)\")\n",
    "print(f\"Duplicate article IDs: {total_duplicates} ({total_duplicates / total_expected_articles * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1918f55",
   "metadata": {},
   "source": [
    "#### Step 2: Converting .txt Files to jsonl Files for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f5027444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full JSON array parse failed, continuing...\n",
      "Parsed using JSON object extraction method (where possible).\n",
      "Converted 203 queries to rankings/gpt4o-mini_sorted_ranking_nl.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- CONFIG --------\n",
    "input_txt_path = Path(\"rankings/gpt4o-mini_sorted_ranking_nl.txt\")\n",
    "output_jsonl_path = Path(\"rankings/gpt4o-mini_sorted_ranking_nl.jsonl\")\n",
    "\n",
    "def extract_ids_any_format(ranked_field):\n",
    "    \"\"\"Extract article IDs from either a list, number string, or mixed format.\"\"\"\n",
    "    if isinstance(ranked_field, list):\n",
    "        blob = \" \".join(str(x) for x in ranked_field)\n",
    "    else:\n",
    "        blob = str(ranked_field)\n",
    "    return re.findall(r\"\\d+\", blob)\n",
    "\n",
    "def get_field(obj, *possible_keys):\n",
    "    for k in possible_keys:\n",
    "        if k in obj:\n",
    "            return obj[k]\n",
    "    return None\n",
    "\n",
    "jsonl_entries = []\n",
    "\n",
    "with open(input_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read().strip()\n",
    "\n",
    "parsed_qids = set()\n",
    "\n",
    "# -------- TRY FULL JSON ARRAY PARSE --------\n",
    "try:\n",
    "    cleaned = re.sub(r\"}\\s*{\", \"},\\n{\", raw_text)\n",
    "    cleaned = re.sub(r\",\\s*]\", \"]\", cleaned)\n",
    "    cleaned = re.sub(r\",\\s*}\", \"}\", cleaned)\n",
    "    if not cleaned.strip().startswith(\"[\"):\n",
    "        cleaned = \"[\" + cleaned + \"]\"\n",
    "    data = json.loads(cleaned)\n",
    "    for obj in data:\n",
    "        qid_raw = get_field(obj, \"query_id\", \"query id\")\n",
    "        ranks_raw = get_field(obj, \"ranked_articles\", \"ranked articles\")\n",
    "        if qid_raw is None:\n",
    "            continue\n",
    "        query_id = str(qid_raw).strip().strip('\"').strip(\"'\")\n",
    "        ranks_list = extract_ids_any_format(ranks_raw)\n",
    "        jsonl_entries.append({\"query_id\": query_id, \"ranks\": ranks_list})\n",
    "        parsed_qids.add(query_id)\n",
    "    print(\"Parsed using full JSON array method.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Full JSON array parse failed, continuing...\")\n",
    "\n",
    "# -------- TRY JSON OBJECT REGEX EXTRACTION --------\n",
    "json_objects = re.findall(r\"\\{.*?\\}\", raw_text, flags=re.DOTALL)\n",
    "for obj_str in json_objects:\n",
    "    try:\n",
    "        obj = json.loads(obj_str)\n",
    "    except json.JSONDecodeError:\n",
    "        continue\n",
    "    qid_raw = get_field(obj, \"query_id\", \"query id\")\n",
    "    ranks_raw = get_field(obj, \"ranked_articles\", \"ranked articles\")\n",
    "    if qid_raw is None:\n",
    "        continue\n",
    "    query_id = str(qid_raw).strip().strip('\"').strip(\"'\")\n",
    "    if query_id in parsed_qids:\n",
    "        continue\n",
    "    ranks_list = extract_ids_any_format(ranks_raw)\n",
    "    jsonl_entries.append({\"query_id\": query_id, \"ranks\": ranks_list})\n",
    "    parsed_qids.add(query_id)\n",
    "print(\"Parsed using JSON object extraction method (where possible).\")\n",
    "\n",
    "# -------- FALLBACK: RAW TEXT SCAN FOR ANY REMAINING QIDS --------\n",
    "current_qid = None\n",
    "current_ids = []\n",
    "for line in raw_text.splitlines():\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    qid_match = re.search(r'\"?query[_ ]id\"?\\s*:\\s*\"?(\\d+)\"?', line, re.IGNORECASE)\n",
    "    if qid_match:\n",
    "        if current_qid and current_qid not in parsed_qids:\n",
    "            jsonl_entries.append({\"query_id\": current_qid, \"ranks\": current_ids})\n",
    "            parsed_qids.add(current_qid)\n",
    "        current_qid = qid_match.group(1)\n",
    "        current_ids = []\n",
    "        continue\n",
    "    ids_in_line = re.findall(r\"\\d+\", line)\n",
    "    if ids_in_line:\n",
    "        current_ids.extend(ids_in_line)\n",
    "# flush last\n",
    "if current_qid and current_qid not in parsed_qids:\n",
    "    jsonl_entries.append({\"query_id\": current_qid, \"ranks\": current_ids})\n",
    "    parsed_qids.add(current_qid)\n",
    "\n",
    "# -------- SAVE JSONL --------\n",
    "with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for entry in jsonl_entries:\n",
    "        f_out.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Converted {len(jsonl_entries)} queries to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9565cf1",
   "metadata": {},
   "source": [
    "#### Step 3: Validating the jsonl Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "b83e477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OVERALL REPORT =====\n",
      "Total queries in hard negatives: 203\n",
      "Total queries in rankings file: 203\n",
      "Missing queries: 0\n",
      "Extra queries: 0\n",
      "Queries with <100 IDs: 33 (16.26%)\n",
      "Queries with >100 IDs: 168 (82.76%)\n",
      "Queries with any issues: 202 (99.51%)\n",
      "Missing article IDs: 10371 (51.09%)\n",
      "Extra (hallucinated) article IDs: 6715 (33.08%)\n",
      "Duplicate article IDs: 2606 (12.84%)\n",
      "\n",
      "Normalized rankings written to: rankings/gpt4o-mini_sorted_ranking_nl_fixed.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# -------- CONFIG --------\n",
    "rankings_jsonl_path = Path(\"rankings/gpt4o-mini_sorted_ranking_nl.jsonl\")\n",
    "hard_negatives_path = Path(\"../sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\")\n",
    "normalized_jsonl_path = Path(\"rankings/gpt4o-mini_sorted_ranking_nl_fixed.jsonl\")  # output\n",
    "\n",
    "PAD_TOKEN = \"PAD\"\n",
    "\n",
    "# -------- LOAD RANKINGS --------\n",
    "rankings = {}\n",
    "with open(rankings_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        rankings[str(entry[\"query_id\"]).strip()] = [str(x).strip() for x in entry[\"ranks\"]]\n",
    "\n",
    "# -------- LOAD HARD NEGATIVES --------\n",
    "hard_negatives = {}\n",
    "with open(hard_negatives_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        hard_negatives[str(entry[\"query_id\"]).strip()] = set(str(doc) for doc in entry[\"candidate_docs\"])\n",
    "\n",
    "# -------- STATS + NORMALIZATION --------\n",
    "total_missing = 0\n",
    "total_extra = 0\n",
    "total_duplicates = 0\n",
    "queries_with_issues = 0\n",
    "queries_lt_100 = 0\n",
    "queries_gt_100 = 0\n",
    "\n",
    "missing_queries = set(hard_negatives.keys()) - set(rankings.keys())\n",
    "extra_queries = set(rankings.keys()) - set(hard_negatives.keys())\n",
    "\n",
    "normalized_entries = []\n",
    "\n",
    "for qid, candidates in hard_negatives.items():\n",
    "    predicted = rankings.get(qid, [])\n",
    "    n_pred = len(predicted)\n",
    "\n",
    "    # Count <100 and >100 BEFORE normalization\n",
    "    if n_pred < 100:\n",
    "        queries_lt_100 += 1\n",
    "    elif n_pred > 100:\n",
    "        queries_gt_100 += 1\n",
    "\n",
    "    # Validation ignores PAD\n",
    "    non_pad = [p for p in predicted if p != PAD_TOKEN]\n",
    "    pred_set = set(non_pad)\n",
    "\n",
    "    missing_ids = candidates - pred_set\n",
    "    extra_ids = pred_set - candidates\n",
    "\n",
    "    # Count duplicates (ignoring PAD)\n",
    "    dupes = [doc for doc, cnt in Counter(non_pad).items() if cnt > 1]\n",
    "\n",
    "    total_missing += len(missing_ids)\n",
    "    total_extra += len(extra_ids)\n",
    "    total_duplicates += len(dupes)\n",
    "\n",
    "    if n_pred != 100 or missing_ids or extra_ids or dupes:\n",
    "        queries_with_issues += 1\n",
    "\n",
    "    # ---- Duplicate removal before normalization ----\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for doc_id in predicted:\n",
    "        if doc_id in seen:\n",
    "            deduped.append(PAD_TOKEN)  # replace duplicate with PAD\n",
    "        else:\n",
    "            deduped.append(doc_id)\n",
    "            seen.add(doc_id)\n",
    "\n",
    "    # ---- Normalize to exactly 100 ----\n",
    "    if len(deduped) > 100:\n",
    "        normalized = deduped[:100]\n",
    "    elif len(deduped) < 100:\n",
    "        normalized = deduped + [PAD_TOKEN] * (100 - len(deduped))\n",
    "    else:\n",
    "        normalized = deduped\n",
    "\n",
    "    normalized_entries.append({\"query_id\": qid, \"ranks\": normalized})\n",
    "\n",
    "# Also include any extra queries present only in rankings\n",
    "for qid in sorted(extra_queries):\n",
    "    pred = rankings[qid]\n",
    "\n",
    "    # Remove duplicates first\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for doc_id in pred:\n",
    "        if doc_id in seen:\n",
    "            deduped.append(PAD_TOKEN)\n",
    "        else:\n",
    "            deduped.append(doc_id)\n",
    "            seen.add(doc_id)\n",
    "\n",
    "    if len(deduped) > 100:\n",
    "        norm = deduped[:100]\n",
    "    else:\n",
    "        norm = deduped + [PAD_TOKEN] * (100 - len(deduped))\n",
    "    normalized_entries.append({\"query_id\": qid, \"ranks\": norm})\n",
    "\n",
    "# -------- WRITE NORMALIZED JSONL --------\n",
    "with open(normalized_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for e in normalized_entries:\n",
    "        f_out.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# -------- REPORT --------\n",
    "total_expected_articles = len(hard_negatives) * 100\n",
    "pct = lambda x, d: (x / d * 100) if d else 0.0\n",
    "\n",
    "print(\"\\n===== OVERALL REPORT =====\")\n",
    "print(f\"Total queries in hard negatives: {len(hard_negatives)}\")\n",
    "print(f\"Total queries in rankings file: {len(rankings)}\")\n",
    "print(f\"Missing queries: {len(missing_queries)}\")\n",
    "print(f\"Extra queries: {len(extra_queries)}\")\n",
    "print(f\"Queries with <100 IDs: {queries_lt_100} ({pct(queries_lt_100, len(hard_negatives)):.2f}%)\")\n",
    "print(f\"Queries with >100 IDs: {queries_gt_100} ({pct(queries_gt_100, len(hard_negatives)):.2f}%)\")\n",
    "print(f\"Queries with any issues: {queries_with_issues} ({pct(queries_with_issues, len(hard_negatives)):.2f}%)\")\n",
    "print(f\"Missing article IDs: {total_missing} ({pct(total_missing, total_expected_articles):.2f}%)\")\n",
    "print(f\"Extra (hallucinated) article IDs: {total_extra} ({pct(total_extra, total_expected_articles):.2f}%)\")\n",
    "print(f\"Duplicate article IDs: {total_duplicates} ({pct(total_duplicates, total_expected_articles):.2f}%)\")\n",
    "print(f\"\\nNormalized rankings written to: {normalized_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ada1f",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "57a96549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded predictions for 203 queries\n",
      "Loaded gold standard for 203 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 19534.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R@1: 0.1572\n",
      "R@5: 0.4061\n",
      "R@10: 0.5237\n",
      "R@20: 0.6701\n",
      "R@50: 0.8529\n",
      "R@100: 1.0066\n",
      "MRR@1: 0.3005\n",
      "MRR@5: 0.4089\n",
      "MRR@10: 0.4214\n",
      "MRR@20: 0.4299\n",
      "MRR@50: 0.4335\n",
      "MRR@100: 0.4346\n",
      "MAP@1: 0.1572\n",
      "MAP@5: 0.2725\n",
      "MAP@10: 0.3016\n",
      "MAP@20: 0.3208\n",
      "MAP@50: 0.3334\n",
      "MAP@100: 0.3393\n",
      "nDCG@1: 0.3005\n",
      "nDCG@5: 0.3463\n",
      "nDCG@10: 0.3905\n",
      "nDCG@20: 0.4376\n",
      "nDCG@50: 0.4858\n",
      "nDCG@100: 0.5209\n",
      "\n",
      "Evaluation results saved to: evaluation/eval_sorted_ranking_jina.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "\n",
    "'''\n",
    ">>> scored_ranking Files\n",
    "'''\n",
    "#predictions_json = Path(\"rankings/scored/json/gpt4.1.mini.ranks.nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/scored/json/gpt4o.mini.ranks.nl.jsonl\") \n",
    "\n",
    "#predictions_json = Path(\"rankings/scored/json/gemini2.5.flash.ranks.nl.jsonl\") \n",
    "\n",
    "#predictions_json = Path(\"rankings/scored/json/qwen3.235b.ranks.nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/scored/json/llama3.3.70b.ranks.nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/scored/json/llama4.scout.ranks.nl.jsonl\") \n",
    "\n",
    "\n",
    "'''\n",
    ">>> Sorted_ranking Files\n",
    "'''\n",
    "#predictions_json = Path(\"rankings/sorted/json/gemini2.5.flash_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/gpt4.1.mini_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/gpt4o-mini_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/qwen3.235b_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/llama4_scout_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/llama3.3_70b_sorted_ranks_nl.jsonl\") \n",
    "\n",
    "\n",
    "'''\n",
    ">>> Baselines\n",
    "'''\n",
    "#predictions_json = Path(\"rankings/sorted/json/me5_top100_ranks_nl.jsonl\")\n",
    "predictions_json = Path(\"rankings/sorted/json/jina_ranks_nl.jsonl\")\n",
    "\n",
    "\n",
    "gold_json = Path(\"gold_data/gold_standard_nl.json\")\n",
    "output_dir = Path(\"evaluation\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"eval_sorted_ranking_jina.txt\" \n",
    "\n",
    "ks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "# load predictions from JSONL\n",
    "predictions = {}\n",
    "with open(predictions_json, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        predictions[item[\"query_id\"]] = item[\"ranks\"]\n",
    "\n",
    "# load gold\n",
    "with open(gold_json, encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "print(f\"Loaded predictions for {len(predictions)} queries\")\n",
    "print(f\"Loaded gold standard for {len(gold)} queries\")\n",
    "\n",
    "metrics = {f\"R@{k}\": [] for k in ks}\n",
    "metrics.update({f\"MRR@{k}\": [] for k in ks})\n",
    "metrics.update({f\"MAP@{k}\": [] for k in ks})\n",
    "metrics.update({f\"nDCG@{k}\": [] for k in ks})\n",
    "\n",
    "def compute_hits(relevant, predicted, k):\n",
    "    hits = [1 if doc in relevant else 0 for doc in predicted[:k]]\n",
    "    return hits\n",
    "\n",
    "for qid in tqdm(predictions.keys(), desc=\"Evaluating\"):\n",
    "    pred = predictions[qid]\n",
    "    gold_set = set(gold[qid])\n",
    "\n",
    "    for k in ks:\n",
    "        hits = compute_hits(gold_set, pred, k)\n",
    "\n",
    "        recall = sum(hits) / len(gold_set) if gold_set else 0.0\n",
    "        metrics[f\"R@{k}\"].append(recall)\n",
    "\n",
    "        rr = 0.0\n",
    "        for rank, h in enumerate(hits):\n",
    "            if h:\n",
    "                rr = 1.0 / (rank + 1)\n",
    "                break\n",
    "        metrics[f\"MRR@{k}\"].append(rr)\n",
    "\n",
    "        ap = 0.0\n",
    "        hit_count = 0\n",
    "        for rank, h in enumerate(hits):\n",
    "            if h:\n",
    "                hit_count += 1\n",
    "                ap += hit_count / (rank + 1)\n",
    "        ap /= len(gold_set) if gold_set else 1\n",
    "        metrics[f\"MAP@{k}\"].append(ap)\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i, h in enumerate(hits):\n",
    "            if h:\n",
    "                dcg += 1.0 / (math.log2(i + 2))  # +2 because log2(rank+1), and rank = 0-based\n",
    "\n",
    "        ideal_hits = [1] * min(len(gold_set), k)\n",
    "        idcg = sum(1.0 / math.log2(i + 2) for i in range(len(ideal_hits)))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        metrics[f\"nDCG@{k}\"].append(ndcg)\n",
    "\n",
    "print()\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    for m, vals in metrics.items():\n",
    "        line = f\"{m}: {mean(vals):.4f}\"\n",
    "        print(line)\n",
    "        out.write(line + \"\\n\")\n",
    "\n",
    "print(f\"\\nEvaluation results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1c69e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gpt4.1.mini.ranks.nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt4.1.mini.ranks.nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 30799.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gpt4o.mini.ranks.nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt4o.mini.ranks.nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 26982.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gemini2.5.flash.ranks.nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gemini2.5.flash.ranks.nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 29739.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating qwen3.235b.ranks.nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating qwen3.235b.ranks.nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 35445.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating llama3.3.70b.ranks.nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating llama3.3.70b.ranks.nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 35670.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating llama4.scout.ranks.nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating llama4.scout.ranks.nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 33498.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gemini2.5.flash_sorted_ranks_nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gemini2.5.flash_sorted_ranks_nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 36411.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gpt4.1.mini_sorted_ranks_nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt4.1.mini_sorted_ranks_nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 28027.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating gpt4o-mini_sorted_ranks_nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt4o-mini_sorted_ranks_nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 37592.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating qwen3.235b_sorted_ranks_nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating qwen3.235b_sorted_ranks_nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 35910.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating llama4_scout_sorted_ranks_nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating llama4_scout_sorted_ranks_nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 29945.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating llama3.3_70b_sorted_ranks_nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating llama3.3_70b_sorted_ranks_nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 35809.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating me5_top100_ranks_nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating me5_top100_ranks_nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 34988.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating jina_ranks_nl: 203 queries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jina_ranks_nl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:00<00:00, 36006.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Evaluation done for 14 models.\n",
      "TXT table saved to: evaluation/eval_all_models.txt\n",
      "CSV table saved to: evaluation/eval_all_models.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIG ---\n",
    "prediction_files = [\n",
    "    \"rankings/scored/json/gpt4.1.mini.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/gpt4o.mini.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/gemini2.5.flash.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/qwen3.235b.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/llama3.3.70b.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/llama4.scout.ranks.nl.jsonl\",\n",
    "    \"rankings/sorted/json/gemini2.5.flash_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/gpt4.1.mini_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/gpt4o-mini_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/qwen3.235b_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/llama4_scout_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/llama3.3_70b_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/me5_top100_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/jina_ranks_nl.jsonl\"\n",
    "]\n",
    "\n",
    "gold_json = Path(\"gold_data/gold_standard_nl.json\")\n",
    "output_dir = Path(\"evaluation\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_txt = output_dir / \"eval_all_models.txt\"\n",
    "output_csv = output_dir / \"eval_all_models.csv\"\n",
    "\n",
    "ks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "# Load gold\n",
    "with open(gold_json, encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "def compute_hits(relevant, predicted, k):\n",
    "    return [1 if doc in relevant else 0 for doc in predicted[:k]]\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for file_path in prediction_files:\n",
    "    file_path = Path(file_path)\n",
    "    model_name = file_path.stem  # e.g. \"gemini2.5.flash_sorted_ranks_nl\"\n",
    "\n",
    "    # Load predictions\n",
    "    predictions = {}\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            predictions[item[\"query_id\"]] = item[\"ranks\"]\n",
    "\n",
    "    print(f\"Evaluating {model_name}: {len(predictions)} queries loaded.\")\n",
    "\n",
    "    metrics = {f\"R@{k}\": [] for k in ks}\n",
    "    metrics.update({f\"MRR@{k}\": [] for k in ks})\n",
    "    metrics.update({f\"MAP@{k}\": [] for k in ks})\n",
    "    metrics.update({f\"nDCG@{k}\": [] for k in ks})\n",
    "\n",
    "    for qid in tqdm(predictions.keys(), desc=f\"Evaluating {model_name}\"):\n",
    "        pred = predictions[qid]\n",
    "        gold_set = set(gold[qid])\n",
    "\n",
    "        for k in ks:\n",
    "            hits = compute_hits(gold_set, pred, k)\n",
    "\n",
    "            recall = sum(hits) / len(gold_set) if gold_set else 0.0\n",
    "            metrics[f\"R@{k}\"].append(recall)\n",
    "\n",
    "            rr = 0.0\n",
    "            for rank, h in enumerate(hits):\n",
    "                if h:\n",
    "                    rr = 1.0 / (rank + 1)\n",
    "                    break\n",
    "            metrics[f\"MRR@{k}\"].append(rr)\n",
    "\n",
    "            ap = 0.0\n",
    "            hit_count = 0\n",
    "            for rank, h in enumerate(hits):\n",
    "                if h:\n",
    "                    hit_count += 1\n",
    "                    ap += hit_count / (rank + 1)\n",
    "            ap /= len(gold_set) if gold_set else 1\n",
    "            metrics[f\"MAP@{k}\"].append(ap)\n",
    "\n",
    "            dcg = 0.0\n",
    "            for i, h in enumerate(hits):\n",
    "                if h:\n",
    "                    dcg += 1.0 / (math.log2(i + 2))\n",
    "            ideal_hits = [1] * min(len(gold_set), k)\n",
    "            idcg = sum(1.0 / math.log2(i + 2) for i in range(len(ideal_hits)))\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "            metrics[f\"nDCG@{k}\"].append(ndcg)\n",
    "\n",
    "    # Add to results table\n",
    "    results_table.append({\n",
    "        \"Model\": model_name,\n",
    "        \"R@1\": mean(metrics[\"R@1\"]),\n",
    "        \"R@5\": mean(metrics[\"R@5\"]),\n",
    "        \"R@10\": mean(metrics[\"R@10\"]),\n",
    "        \"MRR@10\": mean(metrics[\"MRR@10\"]),\n",
    "        \"MAP@10\": mean(metrics[\"MAP@10\"]),\n",
    "        \"nDCG@1\": mean(metrics[\"nDCG@1\"]),\n",
    "        \"nDCG@10\": mean(metrics[\"nDCG@10\"]),\n",
    "        \"nDCG@100\": mean(metrics[\"nDCG@100\"]),\n",
    "    })\n",
    "\n",
    "# Save TXT\n",
    "with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    header = \"Model\\tR@1\\tR@5\\tR@10\\tMRR@10\\tMAP@10\\tnDCG@1\\tnDCG@10\\tnDCG@100\\n\"\n",
    "    f.write(header)\n",
    "    for row in results_table:\n",
    "        f.write(\n",
    "            f\"{row['Model']}\\t\"\n",
    "            f\"{row['R@1']:.4f}\\t{row['R@5']:.4f}\\t{row['R@10']:.4f}\\t\"\n",
    "            f\"{row['MRR@10']:.4f}\\t{row['MAP@10']:.4f}\\t\"\n",
    "            f\"{row['nDCG@1']:.4f}\\t{row['nDCG@10']:.4f}\\t{row['nDCG@100']:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "# Save CSV\n",
    "df = pd.DataFrame(results_table)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Evaluation done for {len(prediction_files)} models.\")\n",
    "print(f\"TXT table saved to: {output_txt}\")\n",
    "print(f\"CSV table saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428855a",
   "metadata": {},
   "source": [
    "## chacking and post-processing the scored_ranking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "abe65065",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rankings/scored/llama4.scout_score_ranking_nl.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m fixed_entries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m current_entry_lines \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     31\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_legal_document_retrieval/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rankings/scored/llama4.scout_score_ranking_nl.jsonl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "output_path = Path(f\"rankings/scored/llama4.scout_score_ranking_nl.jsonl\")\n",
    "hard_negatives_path = Path(f\"../sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\")\n",
    "fixed_output_path = Path(f\"rankings/scored/llama4.scout_score_ranking_nl_sorted.jsonl\")\n",
    "\n",
    "\n",
    "# === LOAD HARD NEGATIVES (GROUND TRUTH) ===\n",
    "with open(hard_negatives_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    hard_negatives = [json.loads(line) for line in f]\n",
    "\n",
    "expected_query_ids = set()\n",
    "expected_doc_ids_per_query = {}\n",
    "\n",
    "for entry in hard_negatives:\n",
    "    qid = str(entry[\"query_id\"])\n",
    "    expected_query_ids.add(qid)\n",
    "    expected_doc_ids_per_query[qid] = set(entry[\"candidate_docs\"])\n",
    "\n",
    "\n",
    "# === READ AND FIX MALFORMED JSONL ===\n",
    "fixed_entries = []\n",
    "current_entry_lines = []\n",
    "\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith('{') and current_entry_lines:\n",
    "            try:\n",
    "                entry = json.loads(' '.join(current_entry_lines))\n",
    "                fixed_entries.append(entry)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Skipping malformed entry.\")\n",
    "            current_entry_lines = [line]\n",
    "        else:\n",
    "            current_entry_lines.append(line)\n",
    "\n",
    "    # Process last entry\n",
    "    if current_entry_lines:\n",
    "        try:\n",
    "            entry = json.loads(' '.join(current_entry_lines))\n",
    "            fixed_entries.append(entry)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Skipping final malformed entry.\")\n",
    "\n",
    "\n",
    "# === SORT AND VALIDATE ===\n",
    "valid_entries = []\n",
    "\n",
    "for entry in tqdm(fixed_entries, desc=\"Validating and sorting\"):\n",
    "    qid = entry.get(\"query_id\")\n",
    "    scores = entry.get(\"relevance_scores\", {})\n",
    "    if not qid or qid not in expected_query_ids:\n",
    "        print(f\"Skipping unknown or missing query ID: {qid}\")\n",
    "        continue\n",
    "\n",
    "    if set(scores.keys()) != expected_doc_ids_per_query[qid]:\n",
    "        print(f\"Skipping query {qid}: mismatched article IDs.\")\n",
    "        continue\n",
    "\n",
    "    # Sort by score descending\n",
    "        # Convert all scores to integers (handle str/int mix)\n",
    "    try:\n",
    "        int_scores = {doc_id: int(score) for doc_id, score in scores.items()}\n",
    "    except ValueError:\n",
    "        print(f\"Skipping query {qid}: contains non-integer-convertible score.\")\n",
    "        continue\n",
    "\n",
    "    # Sort by descending score\n",
    "    sorted_scores = dict(sorted(int_scores.items(), key=lambda x: -x[1]))\n",
    "    valid_entries.append({\n",
    "        \"query_id\": qid,\n",
    "        \"relevance_scores\": sorted_scores\n",
    "    })\n",
    "\n",
    "\n",
    "# === WRITE FIXED JSONL ===\n",
    "fixed_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(fixed_output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for entry in valid_entries:\n",
    "        f_out.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"\\nFixed and validated output written to: {fixed_output_path}\")\n",
    "\n",
    "# === WRITE SECOND OUTPUT (ONLY RANKED DOC IDS, NO SCORES) ===\n",
    "ranks_only_output_path = Path(\"rankings/scored/llama4.scout.ranks.nl.jsonl\")\n",
    "\n",
    "with open(ranks_only_output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for entry in valid_entries:\n",
    "        qid = entry[\"query_id\"]\n",
    "        ranked_ids = list(entry[\"relevance_scores\"].keys())  # Already sorted\n",
    "        f_out.write(json.dumps({\n",
    "            \"query_id\": qid,\n",
    "            \"ranks\": ranked_ids\n",
    "        }) + \"\\n\")\n",
    "\n",
    "print(f\"Ranks-only output written to: {ranks_only_output_path}\")\n",
    "\n",
    "# === REPORT MISSING QUERIES ===\n",
    "fixed_query_ids = {entry[\"query_id\"] for entry in valid_entries}\n",
    "missing_query_ids = sorted(expected_query_ids - fixed_query_ids)\n",
    "\n",
    "if missing_query_ids:\n",
    "    print(f\"\\nâš  Missing query IDs ({len(missing_query_ids)}):\")\n",
    "    print(missing_query_ids)\n",
    "else:\n",
    "    print(\"\\nAll query IDs accounted for.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "deac2ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSV files saved:\n",
      " - macro_scores.csv\n",
      " - micro_scores.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your evaluation directory\n",
    "evaluation_folder = \"retrievals/evaluation\"\n",
    "\n",
    "# List all .txt files in the folder\n",
    "files = [f for f in os.listdir(evaluation_folder) if f.endswith(\".txt\")]\n",
    "\n",
    "# Prepare lists to collect macro and micro data\n",
    "macro_data = []\n",
    "micro_data = []\n",
    "\n",
    "# Function to clean and round values\n",
    "def parse_score(value):\n",
    "    num = float(value)\n",
    "    if num > 1.0 and num <= 100:\n",
    "        num = num / 1000  # e.g., 322 -> 0.322\n",
    "    return round(num, 2)  # keep 2 digits\n",
    "\n",
    "for filename in files:\n",
    "    filepath = os.path.join(evaluation_folder, filename)\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        # Strip and remove empty lines\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Find the relevant sections\n",
    "    macro_index = lines.index(\"Macro-averaged metrics:\")\n",
    "    micro_index = lines.index(\"Micro-averaged metrics:\")\n",
    "    \n",
    "    macro_precision = parse_score(lines[macro_index + 1].split(\":\")[1].strip())\n",
    "    macro_recall = parse_score(lines[macro_index + 2].split(\":\")[1].strip())\n",
    "    macro_f1 = parse_score(lines[macro_index + 3].split(\":\")[1].strip())\n",
    "    \n",
    "    micro_precision = parse_score(lines[micro_index + 1].split(\":\")[1].strip())\n",
    "    micro_recall = parse_score(lines[micro_index + 2].split(\":\")[1].strip())\n",
    "    micro_f1 = parse_score(lines[micro_index + 3].split(\":\")[1].strip())\n",
    "\n",
    "    # Extract model name and method\n",
    "    name = filename.replace(\".txt\", \"\")\n",
    "    if \"id_retr_\" in name:\n",
    "        method = \"ID-Retrieval\"\n",
    "        model = name.split(\"id_retr_\")[-1]\n",
    "    elif \"bin_retr_\" in name:\n",
    "        method = \"Relevance-Classification\"\n",
    "        model = name.split(\"bin_retr_\")[-1]\n",
    "    else:\n",
    "        method = \"Unknown\"\n",
    "        model = name\n",
    "\n",
    "    # Append to data lists\n",
    "    macro_data.append({\n",
    "        \"Model\": model,\n",
    "        \"Method\": method,\n",
    "        \"Precision\": macro_precision,\n",
    "        \"Recall\": macro_recall,\n",
    "        \"F1-Score\": macro_f1\n",
    "    })\n",
    "\n",
    "    micro_data.append({\n",
    "        \"Model\": model,\n",
    "        \"Method\": method,\n",
    "        \"Precision\": micro_precision,\n",
    "        \"Recall\": micro_recall,\n",
    "        \"F1-Score\": micro_f1\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df_macro = pd.DataFrame(macro_data)\n",
    "df_micro = pd.DataFrame(micro_data)\n",
    "\n",
    "df_macro.to_csv(os.path.join(evaluation_folder, \"macro_scores.csv\"), index=False, float_format=\"%.2f\")\n",
    "df_micro.to_csv(os.path.join(evaluation_folder, \"micro_scores.csv\"), index=False, float_format=\"%.2f\")\n",
    "\n",
    "print(\"âœ… CSV files saved:\")\n",
    "print(\" - macro_scores.csv\")\n",
    "print(\" - micro_scores.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
