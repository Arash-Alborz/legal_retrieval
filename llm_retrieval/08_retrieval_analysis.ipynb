{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ad2356",
   "metadata": {},
   "source": [
    "# Model Comparison (ID-Retrieval vs Relevance-Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "output_folder = \"retrievals/analysis\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "macro_df = pd.read_csv(\"retrievals/evaluation/macro_scores.csv\")\n",
    "micro_df = pd.read_csv(\"retrievals/evaluation/micro_scores.csv\")\n",
    "\n",
    "macro_long = macro_df.melt(id_vars=[\"Model\",\"Method\"],\n",
    "                           value_vars=[\"Precision\",\"Recall\",\"F1-Score\"],\n",
    "                           var_name=\"Metric\", value_name=\"Score\")\n",
    "micro_long = micro_df.melt(id_vars=[\"Model\",\"Method\"],\n",
    "                           value_vars=[\"Precision\",\"Recall\",\"F1-Score\"],\n",
    "                           var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "macro_f1 = macro_long[macro_long[\"Metric\"] == \"F1-Score\"]\n",
    "micro_f1 = micro_long[micro_long[\"Metric\"] == \"F1-Score\"]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=macro_f1, x=\"Model\", y=\"Score\", hue=\"Method\", palette=\"Set2\", errorbar=None, dodge=True)\n",
    "plt.title(\"Macro-Averaged F1-Score per Model\"); plt.ylim(0,1); plt.ylabel(\"F1-Score\"); plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.legend(title=\"Method\", loc=\"upper right\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(output_folder,\"macro_metrics_f1_precision.png\"), dpi=300); plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=micro_f1, x=\"Model\", y=\"Score\", hue=\"Method\", palette=\"Set2\", errorbar=None, dodge=True)\n",
    "plt.title(\"Micro-Averaged F1-Score per Model\"); plt.ylim(0,1); plt.ylabel(\"F1-Score\"); plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.legend(title=\"Method\", loc=\"upper right\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(output_folder,\"micro_metrics_comparison_f1.png\"), dpi=300); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae06a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "macro_path = \"retrievals/evaluation/macro_scores.csv\"\n",
    "micro_path = \"retrievals/evaluation/micro_scores.csv\"\n",
    "\n",
    "name_map = {\n",
    "    \"gemini2.5.flash\": \"Gemini-2.5-flash\",\n",
    "    \"qwen3.235b\": \"Qwen-3-235N\",\n",
    "    \"llama4.scout\": \"Llama-4-Scout\",\n",
    "    \"llama3.3.70b\": \"Llama-3.3-70B\",\n",
    "    \"gpt4o.mini\": \"GPT-4o-mini\",\n",
    "    \"gpt4.1.mini\": \"GPT-4.1-mini\"\n",
    "}\n",
    "\n",
    "def replace_model_names(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'model' in df.columns:\n",
    "        df['model'] = df['model'].replace(name_map)\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            if df[col].astype(str).str.lower().isin(name_map.keys()).any():\n",
    "                df[col] = df[col].replace(name_map)\n",
    "                break\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "replace_model_names(macro_path)\n",
    "replace_model_names(micro_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "output_folder = \"retrievals/analysis\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "macro_df = pd.read_csv(\"retrievals/evaluation/macro_scores.csv\")\n",
    "micro_df = pd.read_csv(\"retrievals/evaluation/micro_scores.csv\")\n",
    "\n",
    "macro_long = macro_df.melt(\n",
    "    id_vars=[\"Model\", \"Method\"],\n",
    "    value_vars=[\"Precision\", \"Recall\", \"F1-Score\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Score\"\n",
    ")\n",
    "micro_long = micro_df.melt(\n",
    "    id_vars=[\"Model\", \"Method\"],\n",
    "    value_vars=[\"Precision\", \"Recall\", \"F1-Score\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Score\"\n",
    ")\n",
    "\n",
    "macro_f1 = macro_long[macro_long[\"Metric\"] == \"Precision\"]\n",
    "micro_f1 = micro_long[micro_long[\"Metric\"] == \"Precision\"]\n",
    "macro_avg_f1 = macro_f1.groupby(\"Method\", as_index=False)[\"Score\"].mean()\n",
    "micro_avg_f1 = micro_f1.groupby(\"Method\", as_index=False)[\"Score\"].mean()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.barplot(\n",
    "    data=macro_avg_f1,\n",
    "    x=\"Method\", y=\"Score\",\n",
    "    palette=\"Set2\",\n",
    "    errorbar=None\n",
    ")\n",
    "plt.title(\"Average Macro Precision per Method\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.legend([], [], frameon=False) \n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder, \"macro_avg_precision_per_method.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.barplot(\n",
    "    data=micro_avg_f1,\n",
    "    x=\"Method\", y=\"Score\",\n",
    "    palette=\"Set2\",\n",
    "    errorbar=None\n",
    ")\n",
    "plt.title(\"Average Micro Precision per Method\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.legend([], [], frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder, \"micro_avg_precision_per_method.png\"), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb4d39e",
   "metadata": {},
   "source": [
    "## Retrieval Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "INPUT_DIR = Path(\"retrievals/json\") \n",
    "GOLD_PATH = Path(\"gold_data/gold_standard_nl.json\") \n",
    "\n",
    "OUTPUT_DIR = Path(\"retrievals/analysis\")\n",
    "OUTPUT_AGG_CSV = OUTPUT_DIR / \"retrieval_counts_agg.csv\"\n",
    "OUTPUT_DETAILED_CSV = OUTPUT_DIR / \"retrieval_counts_detailed.csv\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def parse_model_and_scenario(stem: str):\n",
    "    s = stem.lower()\n",
    "    if \"_id_retrieval_\" in s:\n",
    "        scenario = \"ID-Retrieval\"\n",
    "    elif \"_bin_class_retrieval_\" in s:\n",
    "        scenario = \"Relevance-Classification\"\n",
    "    else:\n",
    "        scenario = \"Unknown\"\n",
    "    return stem, scenario \n",
    "\n",
    "def pretty_model_label(stem: str) -> str:\n",
    "    s = stem.lower()\n",
    "    # normalize separators\n",
    "    s = s.replace(\"__\", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "    if \"qwen\" in s:\n",
    "        return \"Qwen-3-235B\"\n",
    "    if \"gemini\" in s and (\"2.5\" in s or \"2_5\" in s) and \"flash\" in s:\n",
    "        return \"Gemini-2.5-flash\"\n",
    "    if (\"gpt4o\" in s or \"gpt_4o\" in s or \"gpt-4o\" in s) and (\"mini\" in s):\n",
    "        return \"GPT-4o-mini\"\n",
    "    if (\"gpt4.1\" in s or \"gpt_4_1\" in s or \"gpt-4.1\" in s) and (\"mini\" in s):\n",
    "        return \"GPT-4.1-mini\"\n",
    "    if \"llama3\" in s and (\"70b\" in s or \"70_b\" in s or \"3.3\" in s or \"3_3\" in s):\n",
    "        return \"LLaMA-3-70B\"\n",
    "    if (\"llama4\" in s or \"llama_4\" in s) and (\"scout\" in s):\n",
    "        return \"LLaMA-4-Scout\"\n",
    "    return stem\n",
    "\n",
    "def normalize_str(x):\n",
    "    return str(x).strip()\n",
    "\n",
    "with open(GOLD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "gold_norm = {normalize_str(q): set(normalize_str(doc) for doc in docs) for q, docs in gold.items()}\n",
    "\n",
    "files = sorted([p for p in INPUT_DIR.glob(\"*.json\")])\n",
    "if not files:\n",
    "    print(f\"No JSON files found in {INPUT_DIR}\")\n",
    "    raise SystemExit\n",
    "\n",
    "detailed_rows = []\n",
    "\n",
    "for fp in files:\n",
    "    stem = fp.stem\n",
    "    raw_model, scenario = parse_model_and_scenario(stem)\n",
    "    model_label = pretty_model_label(raw_model)\n",
    "\n",
    "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    preds = {normalize_str(q): [normalize_str(x) for x in ids] for q, ids in data.items()}\n",
    "\n",
    "    for qid in gold_norm.keys():\n",
    "        ranks = preds.get(qid, [])\n",
    "        gold_set = gold_norm[qid]\n",
    "\n",
    "        retrieved_count = len(ranks)\n",
    "        unique_ranks = list(dict.fromkeys(ranks))\n",
    "        retrieved_unique_count = len(unique_ranks)\n",
    "\n",
    "        tp_raw = sum(1 for rid in ranks if rid in gold_set)\n",
    "        tp_unique = len(set(unique_ranks) & gold_set)\n",
    "\n",
    "        detailed_rows.append({\n",
    "            \"model\": model_label,   \n",
    "            \"scenario\": scenario,\n",
    "            \"query_id\": qid,\n",
    "            \"retrieved_count\": retrieved_count,\n",
    "            \"retrieved_unique_count\": retrieved_unique_count,\n",
    "            \"gold_count\": len(gold_set),\n",
    "            \"true_pos_count\": tp_raw,\n",
    "            \"true_pos_unique_count\": tp_unique,\n",
    "            \"file\": fp.name,\n",
    "        })\n",
    "\n",
    "df_detail = pd.DataFrame(detailed_rows)\n",
    "\n",
    "agg = (\n",
    "    df_detail\n",
    "    .groupby([\"model\", \"scenario\"], as_index=False)\n",
    "    .agg(\n",
    "        queries=(\"query_id\", \"nunique\"),\n",
    "        avg_retrieved=(\"retrieved_count\", \"mean\"),\n",
    "        avg_retrieved_unique=(\"retrieved_unique_count\", \"mean\"),\n",
    "        avg_gold=(\"gold_count\", \"mean\"),\n",
    "        avg_true_pos=(\"true_pos_count\", \"mean\"),\n",
    "        avg_true_pos_unique=(\"true_pos_unique_count\", \"mean\"),\n",
    "    )\n",
    "    .sort_values([\"model\", \"scenario\"])\n",
    ")\n",
    "\n",
    "df_detail.to_csv(OUTPUT_DETAILED_CSV, index=False, float_format=\"%.2f\")\n",
    "agg.to_csv(OUTPUT_AGG_CSV, index=False, float_format=\"%.2f\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "plot_long = agg.melt(\n",
    "    id_vars=[\"model\", \"scenario\", \"avg_gold\"],\n",
    "    value_vars=[\"avg_retrieved\", \"avg_true_pos\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "plot_long[\"Series\"] = plot_long.apply(\n",
    "    lambda r: f\"{'Retrieved' if r['Metric']=='avg_retrieved' else 'True Pos'} \"\n",
    "              f\"({'ID' if r['scenario']=='ID-Retrieval' else 'RC'})\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "series_order = [\"Retrieved (ID)\", \"Retrieved (RC)\", \"True Pos (ID)\", \"True Pos (RC)\"]\n",
    "\n",
    "models_order = sorted(agg[\"model\"].unique().tolist())\n",
    "plot_long[\"model\"] = pd.Categorical(plot_long[\"model\"], categories=models_order, ordered=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(\n",
    "    data=plot_long,\n",
    "    x=\"model\", y=\"value\", hue=\"Series\",\n",
    "    hue_order=series_order, dodge=True\n",
    ")\n",
    "\n",
    "avg_gold = agg[\"avg_gold\"].mean()\n",
    "ax.axhline(avg_gold, color=\"gray\", linestyle=\"--\", linewidth=1, alpha=0.7)\n",
    "ax.text(len(models_order) - 0.5, avg_gold + 0.1,\n",
    "        f\"Avg. gold = {avg_gold:.2f}\",\n",
    "        color=\"gray\", fontsize=9, ha=\"right\", va=\"bottom\")\n",
    "\n",
    "ax.set_title(\"Avg. Retrieved vs True Positives per Model and Scenario\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Average count\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"retrieved_vs_truepos_grouped_bar.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130fe256",
   "metadata": {},
   "source": [
    "#  F1-score Barplots with Error Bars or Per-Query Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425967a8",
   "metadata": {},
   "source": [
    "### Making json files with per-query metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "gold_path = \"../sampling_hard_negatives/gold_standard_nl.json\"\n",
    "retrievals_folder = \"retrievals/json\"\n",
    "output_folder = \"retrievals/analysis/per_query_scores\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "with open(gold_path) as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "model_files = [f for f in os.listdir(retrievals_folder) if f.endswith(\".json\")]\n",
    "\n",
    "for filename in model_files:\n",
    "    with open(os.path.join(retrievals_folder, filename)) as f:\n",
    "        model_data = json.load(f)\n",
    "\n",
    "    per_query_scores = {}\n",
    "\n",
    "    for query_id, gold_docs in gold_data.items():\n",
    "        gold_set = set(gold_docs)\n",
    "        pred_set = set(model_data.get(query_id, []))\n",
    "\n",
    "        tp = len(gold_set & pred_set)\n",
    "        fp = len(pred_set - gold_set)\n",
    "        fn = len(gold_set - pred_set)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        per_query_scores[query_id] = {\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"f1\": round(f1, 4)\n",
    "        }\n",
    "\n",
    "    model_name = filename.replace(\".json\", \"\").replace(\"evaluation_\", \"\")\n",
    "    output_path = os.path.join(output_folder, f\"{model_name}.json\")\n",
    "    with open(output_path, \"w\") as out_f:\n",
    "        json.dump(per_query_scores, out_f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
