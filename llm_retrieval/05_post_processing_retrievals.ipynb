{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281013a5",
   "metadata": {},
   "source": [
    "### Post processing text output of ID retrievals (gpt, llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0bc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> Script for post processing gpt and llama ID retrievals...\n",
    "It takes text files and converts them to json files for evaluation.\n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_txt = Path(\"retrievals/txt/llama4.scout_id_retrieval_nl.txt\") # change name of the file [llama3.3.70b_id_retrieval_{lang}.txt], [gpt4.1.mini_pw_retrievals_nl.txt], [qwen3-235B_id_retrieval_nl.txt]\n",
    "output_json = Path(\"retrievals/json/llama4.scout_id_retrieval_nl.json\") # output\n",
    "\n",
    "pattern_query = re.compile(r\"^query id:\\s*(\\d+)\")\n",
    "pattern_relevant = re.compile(r\"^relevant articles:\\s*(.*)\")\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "with open(results_txt, encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "i = 0\n",
    "while i < len(lines) - 1:\n",
    "    m_query = pattern_query.match(lines[i])\n",
    "    m_relevant = pattern_relevant.match(lines[i+1])\n",
    "    if m_query and m_relevant:\n",
    "        qid = m_query.group(1)\n",
    "        relevant_articles = [x.strip() for x in m_relevant.group(1).split(\",\") if x.strip()]\n",
    "        results_dict[qid] = relevant_articles\n",
    "        i += 2\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(results_dict, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"results_nl.json written to: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d0a8c",
   "metadata": {},
   "source": [
    "### Post processing gpt binary-classification retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a275681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"retrievals/llama4.scout_bin_full_class_retrievals_nl.jsonl\"\n",
    "output_path = \"retrievals/llama4.scout_bin_class_retrieval_nl_fixed.jsonl\"\n",
    "\n",
    "buffer = \"\"\n",
    "results = []\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        buffer += line\n",
    "        if line.strip().endswith(\"}\"): \n",
    "            try:\n",
    "                obj = json.loads(buffer)\n",
    "\n",
    "                if \"relevance\" in obj and isinstance(obj[\"relevance\"], dict):\n",
    "                    normalized_relevance = {}\n",
    "                    for k, v in obj[\"relevance\"].items():\n",
    "                        str_k = str(k)\n",
    "                        str_v = str(v) if v in [0, 1, \"0\", \"1\"] else \"?\"\n",
    "                        normalized_relevance[str_k] = str_v\n",
    "                    obj[\"relevance\"] = normalized_relevance\n",
    "\n",
    "                results.append(obj)\n",
    "                buffer = \"\"\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                continue  \n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for obj in results:\n",
    "        json_line = json.dumps(obj, ensure_ascii=False)\n",
    "        out_f.write(json_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> Script for checking the LLM output. It checks if LLM changes or added or removed any article ids --> comparing to all hard negatives.\n",
    "It also checks the values if they are only 0 or 1. No other characters or empty values. FOR GPT output...\n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "gpt_output_path = \"retrievals/llama4.scout_bin_class_retrieval_nl_fixed.jsonl\"\n",
    "hard_negatives_path = \"../sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\"\n",
    "\n",
    "# load hard negatives\n",
    "hard_negatives = {}\n",
    "with open(hard_negatives_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        hard_negatives[entry[\"query_id\"]] = set(entry[\"candidate_docs\"])\n",
    "\n",
    "invalid_values_queries = []\n",
    "missing_ids_queries = {}\n",
    "extra_ids_queries = {}\n",
    "valid_queries = []\n",
    "\n",
    "with open(gpt_output_path, encoding=\"utf-8\") as f:\n",
    "    for idx, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Line {idx}: INVALID JSON\")\n",
    "            continue\n",
    "\n",
    "        query_id = obj.get(\"query_id\")\n",
    "        relevance = obj.get(\"relevance\", {})\n",
    "\n",
    "        # check relevance values\n",
    "        invalid_values = [v for v in relevance.values() if v not in (\"0\", \"1\")]\n",
    "        if invalid_values:\n",
    "            invalid_values_queries.append(query_id)\n",
    "\n",
    "        # check candidate IDs\n",
    "        expected_ids = hard_negatives.get(query_id)\n",
    "        if not expected_ids:\n",
    "            continue\n",
    "\n",
    "        actual_ids = set(relevance.keys())\n",
    "\n",
    "        missing = expected_ids - actual_ids\n",
    "        extra = actual_ids - expected_ids\n",
    "\n",
    "        if missing:\n",
    "            missing_ids_queries[query_id] = missing\n",
    "        if extra:\n",
    "            extra_ids_queries[query_id] = extra\n",
    "        if not invalid_values and not missing and not extra:\n",
    "            valid_queries.append(query_id)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total queries processed: {len(valid_queries) + len(invalid_values_queries) + len(missing_ids_queries) + len(extra_ids_queries)}\")\n",
    "print(f\"Fully correct queries: {len(valid_queries)}\")\n",
    "\n",
    "if invalid_values_queries:\n",
    "    print(f\"\\nQueries with invalid relevance values: {invalid_values_queries}\")\n",
    "if missing_ids_queries:\n",
    "    print(\"\\nQueries with missing article IDs:\")\n",
    "    for qid, ids in missing_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Missing IDs: {ids}\")\n",
    "if extra_ids_queries:\n",
    "    print(\"\\nQueries with extra article IDs:\")\n",
    "    for qid, ids in extra_ids_queries.items():\n",
    "        print(f\"  Query {qid}: Extra IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917656c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ">>> Script for post ptocessing the binary classification (0/1) outputs and convert them to json retrievals. Only getting the 1 values --> relevant articles. \n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "input_path = \"retrievals/llama4.scout_bin_class_retrieval_nl_fixed.jsonl\"\n",
    "output_path = \"retrievals/json/llama4.scout_bin_class_retrieval_nl.json\"\n",
    "\n",
    "result = {}\n",
    "\n",
    "with open(input_path, encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        query_id = obj[\"query_id\"]\n",
    "        relevance = obj[\"relevance\"]\n",
    "\n",
    "        relevant_articles = [aid for aid, val in relevance.items() if val == \"1\"]\n",
    "        result[query_id] = relevant_articles\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    json.dump(result, fout, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668091b5",
   "metadata": {},
   "source": [
    "### checking all retrievals, if all queries are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590fb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths — adjust if needed\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5_flash_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4.1.mini_id_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4o.mini_id_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4.1.mini_bin_class_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gpt4o.mini_bin_class_retrievals_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5_flash_pro_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5.flash_bin_class_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/gemini_2.5.pro_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/llama3.3.70b_id_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/llama3.3.70b_bin_class_retrieval_nl.json\")\n",
    "#predictions_json = Path(\"retrievals/json/qwen3.235b_bin_class_retrieval_nl.json\")\n",
    "predictions_json = Path(\"retrievals/json/llama4.scout_bin_class_retrieval_nl.json\")\n",
    "\n",
    "gold_json = Path(\"gold_data/gold_standard_nl.json\")\n",
    "\n",
    "# predictions\n",
    "with open(predictions_json, encoding=\"utf-8\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# gold\n",
    "with open(gold_json, encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "pred_ids = set(predictions.keys())\n",
    "gold_ids = set(gold.keys())\n",
    "\n",
    "missing_in_preds = gold_ids - pred_ids\n",
    "extra_in_preds = pred_ids - gold_ids\n",
    "\n",
    "print(f\"Total gold queries: {len(gold_ids)}\")\n",
    "print(f\"Total predicted queries: {len(pred_ids)}\\n\")\n",
    "\n",
    "if missing_in_preds:\n",
    "    print(f\"Missing in predictions ({len(missing_in_preds)}):\")\n",
    "    for qid in sorted(missing_in_preds):\n",
    "        print(f\"  {qid}\")\n",
    "else:\n",
    "    print(\"All gold query IDs are present in predictions.\")\n",
    "\n",
    "if extra_in_preds:\n",
    "    print(f\"\\nExtra query IDs in predictions ({len(extra_in_preds)}):\")\n",
    "    for qid in sorted(extra_in_preds):\n",
    "        print(f\"  {qid}\")\n",
    "else:\n",
    "    print(\"No extra query IDs in predictions.\")\n",
    "\n",
    "if not missing_in_preds and not extra_in_preds:\n",
    "    print(\"\\nPredictions file matches gold file perfectly.\")\n",
    "else:\n",
    "    print(\"\\nPlease fix mismatches before evaluating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd00e41",
   "metadata": {},
   "source": [
    "## Checking the Output of Sorted Ranked Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1918f55",
   "metadata": {},
   "source": [
    "#### Step 2: Converting .txt Files to jsonl Files for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5027444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- CONFIG --------\n",
    "input_txt_path = Path(\"rankings/gpt4o-mini_sorted_ranking_nl.txt\")\n",
    "output_jsonl_path = Path(\"rankings/gpt4o-mini_sorted_ranking_nl.jsonl\")\n",
    "\n",
    "def extract_ids_any_format(ranked_field):\n",
    "    \"\"\"Extract article IDs from either a list, number string, or mixed format.\"\"\"\n",
    "    if isinstance(ranked_field, list):\n",
    "        blob = \" \".join(str(x) for x in ranked_field)\n",
    "    else:\n",
    "        blob = str(ranked_field)\n",
    "    return re.findall(r\"\\d+\", blob)\n",
    "\n",
    "def get_field(obj, *possible_keys):\n",
    "    for k in possible_keys:\n",
    "        if k in obj:\n",
    "            return obj[k]\n",
    "    return None\n",
    "\n",
    "jsonl_entries = []\n",
    "\n",
    "with open(input_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read().strip()\n",
    "\n",
    "parsed_qids = set()\n",
    "\n",
    "# -------- TRY FULL JSON ARRAY PARSE --------\n",
    "try:\n",
    "    cleaned = re.sub(r\"}\\s*{\", \"},\\n{\", raw_text)\n",
    "    cleaned = re.sub(r\",\\s*]\", \"]\", cleaned)\n",
    "    cleaned = re.sub(r\",\\s*}\", \"}\", cleaned)\n",
    "    if not cleaned.strip().startswith(\"[\"):\n",
    "        cleaned = \"[\" + cleaned + \"]\"\n",
    "    data = json.loads(cleaned)\n",
    "    for obj in data:\n",
    "        qid_raw = get_field(obj, \"query_id\", \"query id\")\n",
    "        ranks_raw = get_field(obj, \"ranked_articles\", \"ranked articles\")\n",
    "        if qid_raw is None:\n",
    "            continue\n",
    "        query_id = str(qid_raw).strip().strip('\"').strip(\"'\")\n",
    "        ranks_list = extract_ids_any_format(ranks_raw)\n",
    "        jsonl_entries.append({\"query_id\": query_id, \"ranks\": ranks_list})\n",
    "        parsed_qids.add(query_id)\n",
    "    print(\"Parsed using full JSON array method.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Full JSON array parse failed, continuing...\")\n",
    "\n",
    "# -------- TRY JSON OBJECT REGEX EXTRACTION --------\n",
    "json_objects = re.findall(r\"\\{.*?\\}\", raw_text, flags=re.DOTALL)\n",
    "for obj_str in json_objects:\n",
    "    try:\n",
    "        obj = json.loads(obj_str)\n",
    "    except json.JSONDecodeError:\n",
    "        continue\n",
    "    qid_raw = get_field(obj, \"query_id\", \"query id\")\n",
    "    ranks_raw = get_field(obj, \"ranked_articles\", \"ranked articles\")\n",
    "    if qid_raw is None:\n",
    "        continue\n",
    "    query_id = str(qid_raw).strip().strip('\"').strip(\"'\")\n",
    "    if query_id in parsed_qids:\n",
    "        continue\n",
    "    ranks_list = extract_ids_any_format(ranks_raw)\n",
    "    jsonl_entries.append({\"query_id\": query_id, \"ranks\": ranks_list})\n",
    "    parsed_qids.add(query_id)\n",
    "print(\"Parsed using JSON object extraction method (where possible).\")\n",
    "\n",
    "# -------- FALLBACK: RAW TEXT SCAN FOR ANY REMAINING QIDS --------\n",
    "current_qid = None\n",
    "current_ids = []\n",
    "for line in raw_text.splitlines():\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    qid_match = re.search(r'\"?query[_ ]id\"?\\s*:\\s*\"?(\\d+)\"?', line, re.IGNORECASE)\n",
    "    if qid_match:\n",
    "        if current_qid and current_qid not in parsed_qids:\n",
    "            jsonl_entries.append({\"query_id\": current_qid, \"ranks\": current_ids})\n",
    "            parsed_qids.add(current_qid)\n",
    "        current_qid = qid_match.group(1)\n",
    "        current_ids = []\n",
    "        continue\n",
    "    ids_in_line = re.findall(r\"\\d+\", line)\n",
    "    if ids_in_line:\n",
    "        current_ids.extend(ids_in_line)\n",
    "# flush last\n",
    "if current_qid and current_qid not in parsed_qids:\n",
    "    jsonl_entries.append({\"query_id\": current_qid, \"ranks\": current_ids})\n",
    "    parsed_qids.add(current_qid)\n",
    "\n",
    "# -------- SAVE JSONL --------\n",
    "with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for entry in jsonl_entries:\n",
    "        f_out.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Converted {len(jsonl_entries)} queries to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ada1f",
   "metadata": {},
   "source": [
    "### Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a96549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "\n",
    "'''\n",
    ">>> scored_ranking Files\n",
    "'''\n",
    "#predictions_json = Path(\"rankings/scored/json/gpt4.1.mini.ranks.nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/scored/json/gpt4o.mini.ranks.nl.jsonl\") \n",
    "\n",
    "#predictions_json = Path(\"rankings/scored/json/gemini2.5.flash.ranks.nl.jsonl\") \n",
    "\n",
    "#predictions_json = Path(\"rankings/scored/json/qwen3.235b.ranks.nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/scored/json/llama3.3.70b.ranks.nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/scored/json/llama4.scout.ranks.nl.jsonl\") \n",
    "\n",
    "\n",
    "'''\n",
    ">>> Sorted_ranking Files\n",
    "'''\n",
    "#predictions_json = Path(\"rankings/sorted/json/gemini2.5.flash_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/gpt4.1.mini_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/gpt4o-mini_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/qwen3.235b_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/llama4_scout_sorted_ranks_nl.jsonl\") \n",
    "#predictions_json = Path(\"rankings/sorted/json/llama3.3_70b_sorted_ranks_nl.jsonl\") \n",
    "\n",
    "\n",
    "'''\n",
    ">>> Baselines\n",
    "'''\n",
    "#predictions_json = Path(\"rankings/sorted/json/me5_top100_ranks_nl.jsonl\")\n",
    "predictions_json = Path(\"rankings/sorted/json/jina_ranks_nl.jsonl\")\n",
    "\n",
    "\n",
    "gold_json = Path(\"gold_data/gold_standard_nl.json\")\n",
    "output_dir = Path(\"evaluation\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"eval_sorted_ranking_jina.txt\" \n",
    "\n",
    "ks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "# load predictions from JSONL\n",
    "predictions = {}\n",
    "with open(predictions_json, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        predictions[item[\"query_id\"]] = item[\"ranks\"]\n",
    "\n",
    "# load gold\n",
    "with open(gold_json, encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "print(f\"Loaded predictions for {len(predictions)} queries\")\n",
    "print(f\"Loaded gold standard for {len(gold)} queries\")\n",
    "\n",
    "metrics = {f\"R@{k}\": [] for k in ks}\n",
    "metrics.update({f\"MRR@{k}\": [] for k in ks})\n",
    "metrics.update({f\"MAP@{k}\": [] for k in ks})\n",
    "metrics.update({f\"nDCG@{k}\": [] for k in ks})\n",
    "\n",
    "def compute_hits(relevant, predicted, k):\n",
    "    hits = [1 if doc in relevant else 0 for doc in predicted[:k]]\n",
    "    return hits\n",
    "\n",
    "for qid in tqdm(predictions.keys(), desc=\"Evaluating\"):\n",
    "    pred = predictions[qid]\n",
    "    gold_set = set(gold[qid])\n",
    "\n",
    "    for k in ks:\n",
    "        hits = compute_hits(gold_set, pred, k)\n",
    "\n",
    "        recall = sum(hits) / len(gold_set) if gold_set else 0.0\n",
    "        metrics[f\"R@{k}\"].append(recall)\n",
    "\n",
    "        rr = 0.0\n",
    "        for rank, h in enumerate(hits):\n",
    "            if h:\n",
    "                rr = 1.0 / (rank + 1)\n",
    "                break\n",
    "        metrics[f\"MRR@{k}\"].append(rr)\n",
    "\n",
    "        ap = 0.0\n",
    "        hit_count = 0\n",
    "        for rank, h in enumerate(hits):\n",
    "            if h:\n",
    "                hit_count += 1\n",
    "                ap += hit_count / (rank + 1)\n",
    "        ap /= len(gold_set) if gold_set else 1\n",
    "        metrics[f\"MAP@{k}\"].append(ap)\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i, h in enumerate(hits):\n",
    "            if h:\n",
    "                dcg += 1.0 / (math.log2(i + 2))  # +2 because log2(rank+1), and rank = 0-based\n",
    "\n",
    "        ideal_hits = [1] * min(len(gold_set), k)\n",
    "        idcg = sum(1.0 / math.log2(i + 2) for i in range(len(ideal_hits)))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        metrics[f\"nDCG@{k}\"].append(ndcg)\n",
    "\n",
    "print()\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    for m, vals in metrics.items():\n",
    "        line = f\"{m}: {mean(vals):.4f}\"\n",
    "        print(line)\n",
    "        out.write(line + \"\\n\")\n",
    "\n",
    "print(f\"\\nEvaluation results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "\n",
    "prediction_files = [\n",
    "    \"rankings/scored/json/gpt4.1.mini.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/gpt4o.mini.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/gemini2.5.flash.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/qwen3.235b.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/llama3.3.70b.ranks.nl.jsonl\",\n",
    "    \"rankings/scored/json/llama4.scout.ranks.nl.jsonl\",\n",
    "    \"rankings/sorted/json/gemini2.5.flash_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/gpt4.1.mini_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/gpt4o-mini_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/qwen3.235b_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/llama4_scout_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/llama3.3_70b_sorted_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/me5_top100_ranks_nl.jsonl\",\n",
    "    \"rankings/sorted/json/jina_ranks_nl.jsonl\"\n",
    "]\n",
    "\n",
    "gold_json = Path(\"gold_data/gold_standard_nl.json\")\n",
    "output_dir = Path(\"evaluation\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_txt = output_dir / \"eval_all_models.txt\"\n",
    "output_csv = output_dir / \"eval_all_models.csv\"\n",
    "\n",
    "ks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "with open(gold_json, encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "def compute_hits(relevant, predicted, k):\n",
    "    return [1 if doc in relevant else 0 for doc in predicted[:k]]\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for file_path in prediction_files:\n",
    "    file_path = Path(file_path)\n",
    "    model_name = file_path.stem \n",
    "\n",
    "    predictions = {}\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            predictions[item[\"query_id\"]] = item[\"ranks\"]\n",
    "\n",
    "    print(f\"Evaluating {model_name}: {len(predictions)} queries loaded.\")\n",
    "\n",
    "    metrics = {f\"R@{k}\": [] for k in ks}\n",
    "    metrics.update({f\"MRR@{k}\": [] for k in ks})\n",
    "    metrics.update({f\"MAP@{k}\": [] for k in ks})\n",
    "    metrics.update({f\"nDCG@{k}\": [] for k in ks})\n",
    "\n",
    "    for qid in tqdm(predictions.keys(), desc=f\"Evaluating {model_name}\"):\n",
    "        pred = predictions[qid]\n",
    "        gold_set = set(gold[qid])\n",
    "\n",
    "        for k in ks:\n",
    "            hits = compute_hits(gold_set, pred, k)\n",
    "\n",
    "            recall = sum(hits) / len(gold_set) if gold_set else 0.0\n",
    "            metrics[f\"R@{k}\"].append(recall)\n",
    "\n",
    "            rr = 0.0\n",
    "            for rank, h in enumerate(hits):\n",
    "                if h:\n",
    "                    rr = 1.0 / (rank + 1)\n",
    "                    break\n",
    "            metrics[f\"MRR@{k}\"].append(rr)\n",
    "\n",
    "            ap = 0.0\n",
    "            hit_count = 0\n",
    "            for rank, h in enumerate(hits):\n",
    "                if h:\n",
    "                    hit_count += 1\n",
    "                    ap += hit_count / (rank + 1)\n",
    "            ap /= len(gold_set) if gold_set else 1\n",
    "            metrics[f\"MAP@{k}\"].append(ap)\n",
    "\n",
    "            dcg = 0.0\n",
    "            for i, h in enumerate(hits):\n",
    "                if h:\n",
    "                    dcg += 1.0 / (math.log2(i + 2))\n",
    "            ideal_hits = [1] * min(len(gold_set), k)\n",
    "            idcg = sum(1.0 / math.log2(i + 2) for i in range(len(ideal_hits)))\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "            metrics[f\"nDCG@{k}\"].append(ndcg)\n",
    "\n",
    "    results_table.append({\n",
    "        \"Model\": model_name,\n",
    "        \"R@1\": mean(metrics[\"R@1\"]),\n",
    "        \"R@5\": mean(metrics[\"R@5\"]),\n",
    "        \"R@10\": mean(metrics[\"R@10\"]),\n",
    "        \"MRR@10\": mean(metrics[\"MRR@10\"]),\n",
    "        \"MAP@10\": mean(metrics[\"MAP@10\"]),\n",
    "        \"nDCG@1\": mean(metrics[\"nDCG@1\"]),\n",
    "        \"nDCG@10\": mean(metrics[\"nDCG@10\"]),\n",
    "        \"nDCG@100\": mean(metrics[\"nDCG@100\"]),\n",
    "    })\n",
    "\n",
    "# Save TXT\n",
    "with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    header = \"Model\\tR@1\\tR@5\\tR@10\\tMRR@10\\tMAP@10\\tnDCG@1\\tnDCG@10\\tnDCG@100\\n\"\n",
    "    f.write(header)\n",
    "    for row in results_table:\n",
    "        f.write(\n",
    "            f\"{row['Model']}\\t\"\n",
    "            f\"{row['R@1']:.4f}\\t{row['R@5']:.4f}\\t{row['R@10']:.4f}\\t\"\n",
    "            f\"{row['MRR@10']:.4f}\\t{row['MAP@10']:.4f}\\t\"\n",
    "            f\"{row['nDCG@1']:.4f}\\t{row['nDCG@10']:.4f}\\t{row['nDCG@100']:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(results_table)\n",
    "df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428855a",
   "metadata": {},
   "source": [
    "## chacking and post-processing the scored_ranking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe65065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_path = Path(f\"rankings/scored/llama4.scout_score_ranking_nl.jsonl\")\n",
    "hard_negatives_path = Path(f\"../sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\")\n",
    "fixed_output_path = Path(f\"rankings/scored/llama4.scout_score_ranking_nl_sorted.jsonl\")\n",
    "\n",
    "\n",
    "with open(hard_negatives_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    hard_negatives = [json.loads(line) for line in f]\n",
    "\n",
    "expected_query_ids = set()\n",
    "expected_doc_ids_per_query = {}\n",
    "\n",
    "for entry in hard_negatives:\n",
    "    qid = str(entry[\"query_id\"])\n",
    "    expected_query_ids.add(qid)\n",
    "    expected_doc_ids_per_query[qid] = set(entry[\"candidate_docs\"])\n",
    "\n",
    "\n",
    "fixed_entries = []\n",
    "current_entry_lines = []\n",
    "\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith('{') and current_entry_lines:\n",
    "            try:\n",
    "                entry = json.loads(' '.join(current_entry_lines))\n",
    "                fixed_entries.append(entry)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Skipping malformed entry.\")\n",
    "            current_entry_lines = [line]\n",
    "        else:\n",
    "            current_entry_lines.append(line)\n",
    "\n",
    "    if current_entry_lines:\n",
    "        try:\n",
    "            entry = json.loads(' '.join(current_entry_lines))\n",
    "            fixed_entries.append(entry)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Skipping final malformed entry.\")\n",
    "\n",
    "valid_entries = []\n",
    "\n",
    "for entry in tqdm(fixed_entries, desc=\"Validating and sorting\"):\n",
    "    qid = entry.get(\"query_id\")\n",
    "    scores = entry.get(\"relevance_scores\", {})\n",
    "    if not qid or qid not in expected_query_ids:\n",
    "        print(f\"Skipping unknown or missing query ID: {qid}\")\n",
    "        continue\n",
    "\n",
    "    if set(scores.keys()) != expected_doc_ids_per_query[qid]:\n",
    "        print(f\"Skipping query {qid}: mismatched article IDs.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        int_scores = {doc_id: int(score) for doc_id, score in scores.items()}\n",
    "    except ValueError:\n",
    "        print(f\"Skipping query {qid}: contains non-integer-convertible score.\")\n",
    "        continue\n",
    "\n",
    "    sorted_scores = dict(sorted(int_scores.items(), key=lambda x: -x[1]))\n",
    "    valid_entries.append({\n",
    "        \"query_id\": qid,\n",
    "        \"relevance_scores\": sorted_scores\n",
    "    })\n",
    "\n",
    "\n",
    "fixed_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(fixed_output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for entry in valid_entries:\n",
    "        f_out.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"\\nFixed and validated output written to: {fixed_output_path}\")\n",
    "\n",
    "ranks_only_output_path = Path(\"rankings/scored/llama4.scout.ranks.nl.jsonl\")\n",
    "\n",
    "with open(ranks_only_output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for entry in valid_entries:\n",
    "        qid = entry[\"query_id\"]\n",
    "        ranked_ids = list(entry[\"relevance_scores\"].keys())  \n",
    "        f_out.write(json.dumps({\n",
    "            \"query_id\": qid,\n",
    "            \"ranks\": ranked_ids\n",
    "        }) + \"\\n\")\n",
    "\n",
    "print(f\"Ranks-only output written to: {ranks_only_output_path}\")\n",
    "\n",
    "fixed_query_ids = {entry[\"query_id\"] for entry in valid_entries}\n",
    "missing_query_ids = sorted(expected_query_ids - fixed_query_ids)\n",
    "\n",
    "if missing_query_ids:\n",
    "    print(f\"\\n⚠ Missing query IDs ({len(missing_query_ids)}):\")\n",
    "    print(missing_query_ids)\n",
    "else:\n",
    "    print(\"\\nAll query IDs accounted for.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "evaluation_folder = \"retrievals/evaluation\"\n",
    "\n",
    "files = [f for f in os.listdir(evaluation_folder) if f.endswith(\".txt\")]\n",
    "\n",
    "macro_data = []\n",
    "micro_data = []\n",
    "\n",
    "def parse_score(value):\n",
    "    num = float(value)\n",
    "    if num > 1.0 and num <= 100:\n",
    "        num = num / 1000  \n",
    "    return round(num, 2) \n",
    "\n",
    "for filename in files:\n",
    "    filepath = os.path.join(evaluation_folder, filename)\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    macro_index = lines.index(\"Macro-averaged metrics:\")\n",
    "    micro_index = lines.index(\"Micro-averaged metrics:\")\n",
    "    \n",
    "    macro_precision = parse_score(lines[macro_index + 1].split(\":\")[1].strip())\n",
    "    macro_recall = parse_score(lines[macro_index + 2].split(\":\")[1].strip())\n",
    "    macro_f1 = parse_score(lines[macro_index + 3].split(\":\")[1].strip())\n",
    "    \n",
    "    micro_precision = parse_score(lines[micro_index + 1].split(\":\")[1].strip())\n",
    "    micro_recall = parse_score(lines[micro_index + 2].split(\":\")[1].strip())\n",
    "    micro_f1 = parse_score(lines[micro_index + 3].split(\":\")[1].strip())\n",
    "\n",
    "    name = filename.replace(\".txt\", \"\")\n",
    "    if \"id_retr_\" in name:\n",
    "        method = \"ID-Retrieval\"\n",
    "        model = name.split(\"id_retr_\")[-1]\n",
    "    elif \"bin_retr_\" in name:\n",
    "        method = \"Relevance-Classification\"\n",
    "        model = name.split(\"bin_retr_\")[-1]\n",
    "    else:\n",
    "        method = \"Unknown\"\n",
    "        model = name\n",
    "\n",
    "    macro_data.append({\n",
    "        \"Model\": model,\n",
    "        \"Method\": method,\n",
    "        \"Precision\": macro_precision,\n",
    "        \"Recall\": macro_recall,\n",
    "        \"F1-Score\": macro_f1\n",
    "    })\n",
    "\n",
    "    micro_data.append({\n",
    "        \"Model\": model,\n",
    "        \"Method\": method,\n",
    "        \"Precision\": micro_precision,\n",
    "        \"Recall\": micro_recall,\n",
    "        \"F1-Score\": micro_f1\n",
    "    })\n",
    "\n",
    "df_macro = pd.DataFrame(macro_data)\n",
    "df_micro = pd.DataFrame(micro_data)\n",
    "\n",
    "df_macro.to_csv(os.path.join(evaluation_folder, \"macro_scores.csv\"), index=False, float_format=\"%.2f\")\n",
    "df_micro.to_csv(os.path.join(evaluation_folder, \"micro_scores.csv\"), index=False, float_format=\"%.2f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
