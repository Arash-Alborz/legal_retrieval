{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a17297",
   "metadata": {},
   "source": [
    "## Interpolated PR-Curve + AUC (CSV # PLots for each model in 2 scenarios + Subplot for all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a1edb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined subplot figure: rankings/analysis/curves/pr_interpolated_compare_all_models.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIG ---\n",
    "GOLD_PATH     = Path(\"gold_data/gold_standard_nl.json\")\n",
    "LISTWISE_DIR  = Path(\"rankings/sorted/json\")\n",
    "POINTWISE_DIR = Path(\"rankings/scored/json\")\n",
    "OUT_DIR       = Path(\"rankings/analysis/curves\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def normalize(x): return str(x).strip()\n",
    "\n",
    "def pretty_model(stem: str) -> str:\n",
    "    s = stem.lower().replace(\"__\",\"_\").replace(\"-\",\"_\")\n",
    "    if \"gemini\" in s and (\"2.5\" in s or \"2_5\") and \"flash\" in s: return \"Gemini-2.5-flash\"\n",
    "    if \"qwen\" in s and (\"235\" in s or \"3_235\" in s or \"3-235\" in s): return \"Qwen-3-235B\"\n",
    "    if (\"gpt4o\" in s or \"gpt_4o\" in s or \"gpt-4o\" in s) and \"mini\" in s: return \"GPT-4o-mini\"\n",
    "    if (\"gpt4.1\" in s or \"gpt_4_1\" in s or \"gpt-4.1\" in s) and \"mini\" in s: return \"GPT-4.1-mini\"\n",
    "    if \"llama3\" in s and (\"70b\" in s or \"3.3\" in s or \"3_3\" in s): return \"LLaMA-3-70B\"\n",
    "    if (\"llama4\" in s or \"llama_4\" in s) and \"scout\" in s: return \"LLaMA-4-Scout\"\n",
    "    return stem\n",
    "\n",
    "def safe_name(name: str) -> str:\n",
    "    return \"\".join(c if c.isalnum() or c in \"-_.\" else \"_\" for c in name)\n",
    "\n",
    "def load_gold(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: \n",
    "        gold = json.load(f)\n",
    "    return {normalize(q): set(normalize(d) for d in docs) for q, docs in gold.items()}\n",
    "\n",
    "def load_predictions_jsonl(path: Path):\n",
    "    preds = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            qid = normalize(obj[\"query_id\"])\n",
    "            ranks = [normalize(x) for x in obj[\"ranks\"]]\n",
    "            preds[qid] = ranks\n",
    "    return preds\n",
    "\n",
    "def pr_points_for_query(ranked, gold_set):\n",
    "    if not gold_set: return [], []\n",
    "    tp = 0\n",
    "    P, R = [], []\n",
    "    for i, doc in enumerate(ranked, 1):\n",
    "        if doc in gold_set: tp += 1\n",
    "        P.append(tp / i)\n",
    "        R.append(tp / len(gold_set))\n",
    "    return P, R\n",
    "\n",
    "def interpolated_11_for_model(preds, gold):\n",
    "    R_levels = [i/10 for i in range(11)]\n",
    "    per_query = []\n",
    "    for qid, gold_set in gold.items():\n",
    "        ranked = preds.get(qid, [])\n",
    "        Pq, Rq = pr_points_for_query(ranked, gold_set)\n",
    "        if not Pq:\n",
    "            continue\n",
    "        pairs = list(zip(Rq, Pq))\n",
    "        interp = []\n",
    "        for r0 in R_levels:\n",
    "            pmax = 0.0\n",
    "            for r_i, p_i in pairs:\n",
    "                if r_i >= r0 and p_i > pmax:\n",
    "                    pmax = p_i\n",
    "            interp.append(pmax)\n",
    "        per_query.append(interp)\n",
    "    if not per_query:\n",
    "        return R_levels, [0.0]*11, 0.0\n",
    "    mean_pts = [sum(col)/len(col) for col in zip(*per_query)]\n",
    "    auc_11pt = sum(mean_pts)/len(mean_pts)\n",
    "    return R_levels, mean_pts, auc_11pt\n",
    "\n",
    "def plot_model_compare(model_name, curves_by_scenario, out_path):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for label, (R, P, auc) in curves_by_scenario.items():\n",
    "        plt.plot(R, P, marker=\"o\", linewidth=2, markersize=4, label=f\"{label} (AUC {auc:.2f})\")\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision (interpolated)\")\n",
    "    plt.title(f\"Precision–Recall (11-pt) — {model_name}\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def write_model_csv(model_name, curves_by_scenario, out_csv: Path):\n",
    "    R_levels = [f\"r={i/10:.1f}\" for i in range(11)]\n",
    "    with open(out_csv, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"model,scenario,\" + \",\".join(R_levels) + \",AUC_11pt\\n\")\n",
    "        for scen, (R, P, auc) in curves_by_scenario.items():\n",
    "            row = [model_name, scen] + [f\"{p:.4f}\" for p in P] + [f\"{auc:.4f}\"]\n",
    "            f.write(\",\".join(row) + \"\\n\")\n",
    "\n",
    "# --- Build curves for both scenarios ---\n",
    "gold = load_gold(GOLD_PATH)\n",
    "by_model = {}\n",
    "\n",
    "# Listwise\n",
    "for fp in sorted(LISTWISE_DIR.glob(\"*.jsonl\")):\n",
    "    model = pretty_model(fp.stem)\n",
    "    preds = load_predictions_jsonl(fp)\n",
    "    R, P, auc = interpolated_11_for_model(preds, gold)\n",
    "    by_model.setdefault(model, {})[\"Listwise\"] = (R, P, auc)\n",
    "\n",
    "# Pseudo-pointwise\n",
    "for fp in sorted(POINTWISE_DIR.glob(\"*.jsonl\")):\n",
    "    model = pretty_model(fp.stem)\n",
    "    preds = load_predictions_jsonl(fp)\n",
    "    R, P, auc = interpolated_11_for_model(preds, gold)\n",
    "    by_model.setdefault(model, {})[\"Pseudo-pointwise\"] = (R, P, auc)\n",
    "\n",
    "# --- Per-model plots ---\n",
    "for model, scen_curves in sorted(by_model.items()):\n",
    "    if not scen_curves:\n",
    "        continue\n",
    "    fname = f\"pr_interpolated_compare_{safe_name(model)}.png\"\n",
    "    plot_model_compare(model, scen_curves, OUT_DIR / fname)\n",
    "    write_model_csv(model, scen_curves, OUT_DIR / (Path(fname).with_suffix(\".csv\")))\n",
    "\n",
    "# --- Combined subplot figure ---\n",
    "n_models = len(by_model)\n",
    "ncols = 3\n",
    "nrows = (n_models + ncols - 1) // ncols\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*5, nrows*4), squeeze=False)\n",
    "\n",
    "for idx, (model, scen_curves) in enumerate(sorted(by_model.items())):\n",
    "    ax = axes.flat[idx]\n",
    "    for label, (R, P, auc) in scen_curves.items():\n",
    "        ax.plot(R, P, marker=\"o\", linewidth=2, markersize=3, label=f\"{label} (AUC {auc:.2f})\")\n",
    "    ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
    "    ax.set_title(model, fontsize=10)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    row = idx // ncols\n",
    "    col = idx % ncols\n",
    "    if col == 0:\n",
    "        ax.set_ylabel(\"Precision\")\n",
    "    if row == nrows - 1:\n",
    "        ax.set_xlabel(\"Recall\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "# Hide empty subplots\n",
    "for ax in axes.flat[len(by_model):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "combined_path = OUT_DIR / \"pr_interpolated_compare_all_models.png\"\n",
    "plt.savefig(combined_path, dpi=300)\n",
    "plt.close()\n",
    "print(f\"Saved combined subplot figure: {combined_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f0936",
   "metadata": {},
   "source": [
    "## Plotting average PR + AUC for all models together in two scenarios for scenario comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33da6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - rankings/analysis/pr_interpolated_average_listwise_vs_pointwise.png\n",
      " - rankings/analysis/pr_interpolated_average_listwise_vs_pointwise.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "GOLD_PATH      = Path(\"gold_data/gold_standard_nl.json\")\n",
    "LISTWISE_DIR   = Path(\"rankings/sorted/json\")   # listwise JSONL files\n",
    "POINTWISE_DIR  = Path(\"rankings/scored/json\")   # pseudo-pointwise JSONL files\n",
    "OUT_DIR        = Path(\"rankings/analysis/\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXCLUDE_BASELINES = True  # set False if you want me5/jina included in the averaging\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def normalize(x): \n",
    "    return str(x).strip()\n",
    "\n",
    "def load_gold(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gold = json.load(f)\n",
    "    return {normalize(q): set(normalize(d) for d in docs) for q, docs in gold.items()}\n",
    "\n",
    "def load_predictions_jsonl(path: Path):\n",
    "    preds = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            qid = normalize(obj[\"query_id\"])\n",
    "            ranks = [normalize(x) for x in obj[\"ranks\"]]\n",
    "            preds[qid] = ranks\n",
    "    return preds\n",
    "\n",
    "def pr_points_for_query(ranked, gold_set):\n",
    "    \"\"\"Raw precision/recall as we scan down a ranked list (for one query).\"\"\"\n",
    "    if not gold_set:\n",
    "        return [], []\n",
    "    tp = 0\n",
    "    precisions, recalls = [], []\n",
    "    G = len(gold_set)\n",
    "    for i, doc in enumerate(ranked, 1):\n",
    "        if doc in gold_set:\n",
    "            tp += 1\n",
    "        precisions.append(tp / i)\n",
    "        recalls.append(tp / G)\n",
    "    return precisions, recalls\n",
    "\n",
    "def interpolated_11_for_model(preds, gold):\n",
    "    \"\"\"Return (R_levels, mean_interpolated_precision, AUC_11pt) macro-averaged across queries.\"\"\"\n",
    "    R = [i/10 for i in range(11)]  # 0.0 .. 1.0\n",
    "    per_query = []\n",
    "    for qid, gold_set in gold.items():\n",
    "        ranked = preds.get(qid, [])\n",
    "        P, r = pr_points_for_query(ranked, gold_set)\n",
    "        if not P:\n",
    "            continue\n",
    "        pairs = list(zip(r, P))\n",
    "        # Interpolated precision at recall r0 = max precision at any recall >= r0\n",
    "        interps = []\n",
    "        for r0 in R:\n",
    "            pmax = 0.0\n",
    "            for rq, pq in pairs:\n",
    "                if rq >= r0 and pq > pmax:\n",
    "                    pmax = pq\n",
    "            interps.append(pmax)\n",
    "        per_query.append(interps)\n",
    "    if not per_query:\n",
    "        return R, [0.0]*11, 0.0\n",
    "    mean_pts = [sum(col)/len(col) for col in zip(*per_query)]\n",
    "    auc = sum(mean_pts) / len(mean_pts)  # mean of the 11 interpolated points\n",
    "    return R, mean_pts, auc\n",
    "\n",
    "def collect_model_curves(folder: Path, gold, exclude_baselines=True):\n",
    "    \"\"\"Compute 11-pt curve for each file in folder. Returns list of precision arrays.\"\"\"\n",
    "    curves = []\n",
    "    for fp in sorted(folder.glob(\"*.jsonl\")):\n",
    "        name = fp.name.lower()\n",
    "        if exclude_baselines and (\"me5\" in name or \"jina\" in name):\n",
    "            continue\n",
    "        preds = load_predictions_jsonl(fp)\n",
    "        _, P, _ = interpolated_11_for_model(preds, gold)\n",
    "        curves.append(P)\n",
    "    return curves\n",
    "\n",
    "def average_curves(curve_list):\n",
    "    \"\"\"Column-wise average of multiple precision arrays (each length 11).\"\"\"\n",
    "    if not curve_list:\n",
    "        return [0.0]*11\n",
    "    cols = list(zip(*curve_list))\n",
    "    return [sum(c)/len(c) for c in cols]\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "gold = load_gold(GOLD_PATH)\n",
    "\n",
    "# Collect per-model curves (each a list of 11 precisions) for both scenarios\n",
    "listwise_curves  = collect_model_curves(LISTWISE_DIR,  gold, exclude_baselines=EXCLUDE_BASELINES)\n",
    "pointwise_curves = collect_model_curves(POINTWISE_DIR, gold, exclude_baselines=EXCLUDE_BASELINES)\n",
    "\n",
    "R = [i/10 for i in range(11)]\n",
    "avg_listwise  = average_curves(listwise_curves)\n",
    "avg_pointwise = average_curves(pointwise_curves)\n",
    "\n",
    "auc_listwise  = sum(avg_listwise)  / len(avg_listwise)\n",
    "auc_pointwise = sum(avg_pointwise) / len(avg_pointwise)\n",
    "\n",
    "# ---------- PLOT ----------\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(R, avg_listwise,  marker=\"o\", label=f\"Listwise (AUC {auc_listwise:.2f})\")\n",
    "plt.plot(R, avg_pointwise, marker=\"o\", label=f\"Pseudo-pointwise (AUC {auc_pointwise:.2f})\")\n",
    "plt.xlim(0,1); plt.ylim(0,1)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision (11-pt interpolated)\")\n",
    "plt.title(\"Average Precision–Recall (11-pt) across models\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "avg_png = OUT_DIR / \"pr_interpolated_average_listwise_vs_pointwise.png\"\n",
    "plt.savefig(avg_png, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ---------- CSV ----------\n",
    "avg_csv = OUT_DIR / \"pr_interpolated_average_listwise_vs_pointwise.csv\"\n",
    "with open(avg_csv, \"w\", encoding=\"utf-8\") as f:\n",
    "    header = [\"scenario\"] + [f\"r={r:.1f}\" for r in R] + [\"AUC_11pt\"]\n",
    "    f.write(\",\".join(header) + \"\\n\")\n",
    "    f.write(\",\".join([\"Listwise\"]        + [f\"{p:.4f}\" for p in avg_listwise]  + [f\"{auc_listwise:.4f}\"])  + \"\\n\")\n",
    "    f.write(\",\".join([\"Pseudo-pointwise\"]+ [f\"{p:.4f}\" for p in avg_pointwise] + [f\"{auc_pointwise:.4f}\"]) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", avg_png)\n",
    "print(\" -\", avg_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fe60b",
   "metadata": {},
   "source": [
    "## Extra analysis experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107f457c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Listwise] saved: pr_interpolated_listwise.png, pr_interpolated_listwise.csv in rankings/analysis/pr_curves\n",
      "[Pseudo-pointwise] saved: pr_interpolated_pseudo-pointwise.png, pr_interpolated_pseudo-pointwise.csv in rankings/analysis/pr_curves\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIG ---\n",
    "GOLD_PATH = Path(\"gold_data/gold_standard_nl.json\")\n",
    "LISTWISE_DIR  = Path(\"rankings/sorted/json\")\n",
    "POINTWISE_DIR = Path(\"rankings/scored/json\")\n",
    "OUT_DIR = Path(\"rankings/analysis/pr_curves\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def normalize(x): return str(x).strip()\n",
    "\n",
    "def pretty_model(stem: str) -> str:\n",
    "    s = stem.lower().replace(\"__\",\"_\").replace(\"-\",\"_\")\n",
    "    if \"gemini\" in s and (\"2.5\" in s or \"2_5\") and \"flash\" in s: return \"Gemini-2.5-flash\"\n",
    "    if \"qwen\" in s and \"235\" in s: return \"Qwen-3-235B\"\n",
    "    if \"gpt4o\" in s and \"mini\" in s: return \"GPT-4o-mini\"\n",
    "    if (\"gpt4.1\" in s or \"gpt_4_1\" in s) and \"mini\" in s: return \"GPT-4.1-mini\"\n",
    "    if \"llama3\" in s and (\"70b\" in s or \"3.3\" in s): return \"LLaMA-3-70B\"\n",
    "    if \"llama4\" in s and \"scout\" in s: return \"LLaMA-4-Scout\"\n",
    "    return stem\n",
    "\n",
    "def load_gold(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: gold = json.load(f)\n",
    "    return {normalize(q): set(normalize(d) for d in docs) for q, docs in gold.items()}\n",
    "\n",
    "def load_predictions_jsonl(path: Path):\n",
    "    preds = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            qid = normalize(obj[\"query_id\"])\n",
    "            ranks = [normalize(x) for x in obj[\"ranks\"]]\n",
    "            preds[qid] = ranks\n",
    "    return preds\n",
    "\n",
    "def pr_points_for_query(ranked, gold_set):\n",
    "    \"\"\"Return raw precision/recall points as we scan down the ranked list.\"\"\"\n",
    "    tp = 0\n",
    "    precisions, recalls = [], []\n",
    "    if len(gold_set) == 0:  # skip empty-gold queries\n",
    "        return precisions, recalls\n",
    "    for i, doc in enumerate(ranked, 1):\n",
    "        if doc in gold_set: tp += 1\n",
    "        precisions.append(tp / i)\n",
    "        recalls.append(tp / len(gold_set))\n",
    "    return precisions, recalls\n",
    "\n",
    "def interpolated_11_for_model(preds, gold):\n",
    "    \"\"\"11-point interpolated precision (macro-averaged across queries).\"\"\"\n",
    "    R = [i/10 for i in range(11)]  # 0.0 ... 1.0\n",
    "    per_query = []\n",
    "    for qid, gold_set in gold.items():\n",
    "        ranked = preds.get(qid, [])\n",
    "        P, r = pr_points_for_query(ranked, gold_set)\n",
    "        if not P: \n",
    "            continue\n",
    "        pairs = list(zip(r, P))\n",
    "        # for each recall level r0, take max precision at any recall >= r0\n",
    "        interps = []\n",
    "        for r0 in R:\n",
    "            pmax = 0.0\n",
    "            for Rq, Pq in pairs:\n",
    "                if Rq >= r0 and Pq > pmax:\n",
    "                    pmax = Pq\n",
    "            interps.append(pmax)\n",
    "        per_query.append(interps)\n",
    "    if not per_query:\n",
    "        return R, [0.0]*11, 0.0\n",
    "    # macro average across queries\n",
    "    mean_pts = [sum(col)/len(col) for col in zip(*per_query)]\n",
    "    auc = sum(mean_pts)/len(mean_pts)  # mean of 11 points\n",
    "    return R, mean_pts, auc\n",
    "\n",
    "def plot_curves(curves, title, out_path):\n",
    "    plt.figure(figsize=(9,6))\n",
    "    for model, (R, P, auc) in curves.items():\n",
    "        plt.plot(R, P, label=f\"{model} (AUC {auc:.2f})\")\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision (interpolated)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend(title=\"Models\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def write_csv(curves, out_csv: Path):\n",
    "    # rows: model, r0..r10, AUC\n",
    "    R_levels = [i/10 for i in range(11)]\n",
    "    with open(out_csv, \"w\", encoding=\"utf-8\") as f:\n",
    "        header = [\"model\"] + [f\"r={r:.1f}\" for r in R_levels] + [\"AUC_11pt\"]\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for model, (R, P, auc) in curves.items():\n",
    "            row = [model] + [f\"{p:.4f}\" for p in P] + [f\"{auc:.4f}\"]\n",
    "            f.write(\",\".join(row) + \"\\n\")\n",
    "\n",
    "# --- Run for both scenarios ---\n",
    "gold = load_gold(GOLD_PATH)\n",
    "\n",
    "for scenario_name, folder in [(\"Listwise\", LISTWISE_DIR), (\"Pseudo-pointwise\", POINTWISE_DIR)]:\n",
    "    curves = {}\n",
    "    for fp in sorted(folder.glob(\"*.jsonl\")):\n",
    "        model = pretty_model(fp.stem)\n",
    "        preds = load_predictions_jsonl(fp)\n",
    "        R, P, auc = interpolated_11_for_model(preds, gold)\n",
    "        curves[model] = (R, P, auc)\n",
    "\n",
    "    if not curves:\n",
    "        print(f\"No JSONL files found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    # plot & csv\n",
    "    png = OUT_DIR / f\"pr_interpolated_{scenario_name.lower().replace(' ','_')}.png\"\n",
    "    csv = OUT_DIR / f\"pr_interpolated_{scenario_name.lower().replace(' ','_')}.csv\"\n",
    "    plot_curves(curves, f\"Precision–Recall (11‑point interpolated) — {scenario_name}\", png)\n",
    "    write_csv(curves, csv)\n",
    "    print(f\"[{scenario_name}] saved: {png.name}, {csv.name} in {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb88b4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined delta plot to rankings/analysis/delta_plots_seaborn/delta_all_metrics.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_CSV = Path(\"rankings/eval_all_sorted_models.csv\")\n",
    "OUT_DIR = Path(\"rankings/analysis/delta_plots_seaborn\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# --- Extract base model name ---\n",
    "def extract_base_model(name):\n",
    "    name = name.lower()\n",
    "    name = name.replace(\"_\", \".\")\n",
    "    name = name.replace(\".sorted.ranks.\", \".\")\n",
    "    name = name.replace(\".ranks.\", \".\")\n",
    "    name = name.replace(\"_sorted_ranks_\", \".\")\n",
    "    name = name.replace(\"_ranks_\", \".\")\n",
    "    name = re.sub(r\"\\.nl$\", \"\", name)\n",
    "    return name\n",
    "\n",
    "df[\"BaseModel\"] = df[\"Model\"].apply(extract_base_model)\n",
    "\n",
    "# --- Detect scenario ---\n",
    "def detect_scenario(name):\n",
    "    name = name.lower()\n",
    "    if \"sorted\" in name:\n",
    "        return \"Listwise\"\n",
    "    elif \"ranks\" in name:\n",
    "        return \"Pointwise\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "df[\"Scenario\"] = df[\"Model\"].apply(detect_scenario)\n",
    "\n",
    "# --- Split ---\n",
    "listwise_df = df[df[\"Scenario\"] == \"Listwise\"].copy()\n",
    "pointwise_df = df[df[\"Scenario\"] == \"Pointwise\"].copy()\n",
    "\n",
    "# --- Merge ---\n",
    "merged = pd.merge(\n",
    "    listwise_df,\n",
    "    pointwise_df,\n",
    "    on=\"BaseModel\",\n",
    "    suffixes=(\"_listwise\", \"_pointwise\")\n",
    ")\n",
    "\n",
    "# --- Metrics ---\n",
    "metrics = [\"R@5\", \"R@10\", \"MAP@10\", \"MRR@10\", \"nDCG@10\", \"nDCG@100\"]\n",
    "\n",
    "# Compute deltas\n",
    "for m in metrics:\n",
    "    merged[f\"delta_{m}\"] = merged[f\"{m}_pointwise\"] - merged[f\"{m}_listwise\"]\n",
    "\n",
    "# --- Melt for one combined plot ---\n",
    "delta_cols = [f\"delta_{m}\" for m in metrics]\n",
    "plot_df = merged.melt(id_vars=[\"BaseModel\"], value_vars=delta_cols,\n",
    "                      var_name=\"Metric\", value_name=\"Delta\")\n",
    "plot_df[\"Metric\"] = plot_df[\"Metric\"].str.replace(\"delta_\", \"\")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=plot_df, x=\"BaseModel\", y=\"Delta\", hue=\"Metric\")\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.ylabel(\"Δ (Pointwise - Listwise)\")\n",
    "plt.title(\"Delta across all metrics between Pointwise and Listwise Ranking\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend(title=\"Metric\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"delta_all_metrics.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved combined delta plot to {OUT_DIR / 'delta_all_metrics.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a94da6",
   "metadata": {},
   "source": [
    "## Scatterplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a646d7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot saved as ranking_comparison_scatterplot.png\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data from by_model dictionary\n",
    "auc_data = []\n",
    "for model, scen_curves in by_model.items():\n",
    "    if \"Listwise\" in scen_curves and \"Pseudo-pointwise\" in scen_curves:\n",
    "        listwise_auc = scen_curves[\"Listwise\"][2]\n",
    "        pseudo_auc = scen_curves[\"Pseudo-pointwise\"][2]\n",
    "        auc_data.append((model, listwise_auc, pseudo_auc))\n",
    "\n",
    "df_auc = pd.DataFrame(auc_data, columns=[\"Model\", \"Listwise_AUC\", \"Pseudo_AUC\"])\n",
    "\n",
    "# Create scatter plot with seaborn\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_auc,\n",
    "    x=\"Listwise_AUC\", y=\"Pseudo_AUC\",\n",
    "    hue=\"Model\", s=80, palette=\"tab10\"\n",
    ")\n",
    "\n",
    "# Add diagonal line\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# Annotate each point with model name\n",
    "for _, row in df_auc.iterrows():\n",
    "    plt.text(\n",
    "        row[\"Listwise_AUC\"] + 0.005, row[\"Pseudo_AUC\"] + 0.005,\n",
    "        row[\"Model\"], fontsize=8\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Listwise AUC (11-pt Interpolated PR)\")\n",
    "plt.ylabel(\"Pseudo-pointwise AUC (11-pt Interpolated PR)\")\n",
    "plt.title(\"Listwise vs. Pseudo-pointwise Ranking — AUC Comparison\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\"rankings/ranking_comparison_scatterplot.png\", dpi=300)\n",
    "plt.close()\n",
    "print(\"Scatter plot saved as ranking_comparison_scatterplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4651e545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heatmap as ranking_comparison_auc_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Prepare AUC data ---\n",
    "auc_rows = []\n",
    "for model, scen_curves in by_model.items():\n",
    "    if model not in [\"mE5large-instruct\", \"jina-embeddings-v3\"]:  # exclude dense models\n",
    "        for scenario, (R, P, auc) in scen_curves.items():\n",
    "            if scenario in [\"Listwise\", \"Pseudo-pointwise\"]:\n",
    "                auc_rows.append({\n",
    "                    \"Model\": model,\n",
    "                    \"Ranking Method\": scenario + \"-Ranking\",\n",
    "                    \"AUC\": round(auc * 100, 2)  # percentage for readability\n",
    "                })\n",
    "\n",
    "df_auc = pd.DataFrame(auc_rows)\n",
    "\n",
    "# Pivot for heatmap\n",
    "heatmap_data = df_auc.pivot(index=\"Model\", columns=\"Ranking Method\", values=\"AUC\")\n",
    "\n",
    "# Match order like your current heatmap\n",
    "model_order = [\n",
    "    \"Gemini-2.5-flash\",\n",
    "    \"Qwen-3-235B\",\n",
    "    \"GPT-4.1-mini\",\n",
    "    \"GPT-4o-mini\",\n",
    "    \"LLaMA-4-Scout\",\n",
    "    \"LLaMA-3-70B\"\n",
    "]\n",
    "heatmap_data = heatmap_data.reindex(model_order)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "    cbar_kws={'label': 'AUC (11-pt Interpolated PR)'}\n",
    ")\n",
    "\n",
    "plt.title(\"AUC Heatmap by Model and Ranking Method\")\n",
    "plt.xlabel(\"Ranking Method\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(\"rankings/ranking_comparison_auc_heatmap.png\", dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved heatmap as ranking_comparison_auc_heatmap.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
