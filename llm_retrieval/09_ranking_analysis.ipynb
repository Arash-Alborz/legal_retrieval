{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a17297",
   "metadata": {},
   "source": [
    "## Interpolated PR-Curve + AUC (CSV # PLots for each model in 2 scenarios + Subplot for all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1edb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GOLD_PATH     = Path(\"gold_data/gold_standard_nl.json\")\n",
    "LISTWISE_DIR  = Path(\"rankings/sorted/json\")\n",
    "POINTWISE_DIR = Path(\"rankings/scored/json\")\n",
    "OUT_DIR       = Path(\"rankings/analysis/curves\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def normalize(x): return str(x).strip()\n",
    "\n",
    "def pretty_model(stem: str) -> str:\n",
    "    s = stem.lower().replace(\"__\",\"_\").replace(\"-\",\"_\")\n",
    "    if \"gemini\" in s and (\"2.5\" in s or \"2_5\") and \"flash\" in s: return \"Gemini-2.5-flash\"\n",
    "    if \"qwen\" in s and (\"235\" in s or \"3_235\" in s or \"3-235\" in s): return \"Qwen-3-235B\"\n",
    "    if (\"gpt4o\" in s or \"gpt_4o\" in s or \"gpt-4o\" in s) and \"mini\" in s: return \"GPT-4o-mini\"\n",
    "    if (\"gpt4.1\" in s or \"gpt_4_1\" in s or \"gpt-4.1\" in s) and \"mini\" in s: return \"GPT-4.1-mini\"\n",
    "    if \"llama3\" in s and (\"70b\" in s or \"3.3\" in s or \"3_3\" in s): return \"LLaMA-3-70B\"\n",
    "    if (\"llama4\" in s or \"llama_4\" in s) and \"scout\" in s: return \"LLaMA-4-Scout\"\n",
    "    return stem\n",
    "\n",
    "def safe_name(name: str) -> str:\n",
    "    return \"\".join(c if c.isalnum() or c in \"-_.\" else \"_\" for c in name)\n",
    "\n",
    "def load_gold(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f: \n",
    "        gold = json.load(f)\n",
    "    return {normalize(q): set(normalize(d) for d in docs) for q, docs in gold.items()}\n",
    "\n",
    "def load_predictions_jsonl(path: Path):\n",
    "    preds = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            qid = normalize(obj[\"query_id\"])\n",
    "            ranks = [normalize(x) for x in obj[\"ranks\"]]\n",
    "            preds[qid] = ranks\n",
    "    return preds\n",
    "\n",
    "def pr_points_for_query(ranked, gold_set):\n",
    "    if not gold_set: return [], []\n",
    "    tp = 0\n",
    "    P, R = [], []\n",
    "    for i, doc in enumerate(ranked, 1):\n",
    "        if doc in gold_set: tp += 1\n",
    "        P.append(tp / i)\n",
    "        R.append(tp / len(gold_set))\n",
    "    return P, R\n",
    "\n",
    "def interpolated_11_for_model(preds, gold):\n",
    "    R_levels = [i/10 for i in range(11)]\n",
    "    per_query = []\n",
    "    for qid, gold_set in gold.items():\n",
    "        ranked = preds.get(qid, [])\n",
    "        Pq, Rq = pr_points_for_query(ranked, gold_set)\n",
    "        if not Pq:\n",
    "            continue\n",
    "        pairs = list(zip(Rq, Pq))\n",
    "        interp = []\n",
    "        for r0 in R_levels:\n",
    "            pmax = 0.0\n",
    "            for r_i, p_i in pairs:\n",
    "                if r_i >= r0 and p_i > pmax:\n",
    "                    pmax = p_i\n",
    "            interp.append(pmax)\n",
    "        per_query.append(interp)\n",
    "    if not per_query:\n",
    "        return R_levels, [0.0]*11, 0.0\n",
    "    mean_pts = [sum(col)/len(col) for col in zip(*per_query)]\n",
    "    auc_11pt = sum(mean_pts)/len(mean_pts)\n",
    "    return R_levels, mean_pts, auc_11pt\n",
    "\n",
    "def plot_model_compare(model_name, curves_by_scenario, out_path):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for label, (R, P, auc) in curves_by_scenario.items():\n",
    "        plt.plot(R, P, marker=\"o\", linewidth=2, markersize=4, label=f\"{label} (AUC {auc:.2f})\")\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision (interpolated)\")\n",
    "    plt.title(f\"Precision–Recall (11-pt) — {model_name}\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def write_model_csv(model_name, curves_by_scenario, out_csv: Path):\n",
    "    R_levels = [f\"r={i/10:.1f}\" for i in range(11)]\n",
    "    with open(out_csv, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"model,scenario,\" + \",\".join(R_levels) + \",AUC_11pt\\n\")\n",
    "        for scen, (R, P, auc) in curves_by_scenario.items():\n",
    "            row = [model_name, scen] + [f\"{p:.4f}\" for p in P] + [f\"{auc:.4f}\"]\n",
    "            f.write(\",\".join(row) + \"\\n\")\n",
    "\n",
    "gold = load_gold(GOLD_PATH)\n",
    "by_model = {}\n",
    "\n",
    "# Listwise\n",
    "for fp in sorted(LISTWISE_DIR.glob(\"*.jsonl\")):\n",
    "    model = pretty_model(fp.stem)\n",
    "    preds = load_predictions_jsonl(fp)\n",
    "    R, P, auc = interpolated_11_for_model(preds, gold)\n",
    "    by_model.setdefault(model, {})[\"Listwise\"] = (R, P, auc)\n",
    "\n",
    "# Pseudo-pointwise\n",
    "for fp in sorted(POINTWISE_DIR.glob(\"*.jsonl\")):\n",
    "    model = pretty_model(fp.stem)\n",
    "    preds = load_predictions_jsonl(fp)\n",
    "    R, P, auc = interpolated_11_for_model(preds, gold)\n",
    "    by_model.setdefault(model, {})[\"Pseudo-pointwise\"] = (R, P, auc)\n",
    "\n",
    "for model, scen_curves in sorted(by_model.items()):\n",
    "    if not scen_curves:\n",
    "        continue\n",
    "    fname = f\"pr_interpolated_compare_{safe_name(model)}.png\"\n",
    "    plot_model_compare(model, scen_curves, OUT_DIR / fname)\n",
    "    write_model_csv(model, scen_curves, OUT_DIR / (Path(fname).with_suffix(\".csv\")))\n",
    "\n",
    "n_models = len(by_model)\n",
    "ncols = 3\n",
    "nrows = (n_models + ncols - 1) // ncols\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*5, nrows*4), squeeze=False)\n",
    "\n",
    "for idx, (model, scen_curves) in enumerate(sorted(by_model.items())):\n",
    "    ax = axes.flat[idx]\n",
    "    for label, (R, P, auc) in scen_curves.items():\n",
    "        ax.plot(R, P, marker=\"o\", linewidth=2, markersize=3, label=f\"{label} (AUC {auc:.2f})\")\n",
    "    ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
    "    ax.set_title(model, fontsize=10)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    row = idx // ncols\n",
    "    col = idx % ncols\n",
    "    if col == 0:\n",
    "        ax.set_ylabel(\"Precision\")\n",
    "    if row == nrows - 1:\n",
    "        ax.set_xlabel(\"Recall\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "for ax in axes.flat[len(by_model):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "combined_path = OUT_DIR / \"pr_interpolated_compare_all_models.png\"\n",
    "plt.savefig(combined_path, dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f0936",
   "metadata": {},
   "source": [
    "## Plotting average PR + AUC for all models together in two scenarios for scenario comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33da6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GOLD_PATH      = Path(\"gold_data/gold_standard_nl.json\")\n",
    "LISTWISE_DIR   = Path(\"rankings/sorted/json\")   # listwise JSONL files\n",
    "POINTWISE_DIR  = Path(\"rankings/scored/json\")   # pseudo-pointwise JSONL files\n",
    "OUT_DIR        = Path(\"rankings/analysis/\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXCLUDE_BASELINES = True  \n",
    "\n",
    "def normalize(x): \n",
    "    return str(x).strip()\n",
    "\n",
    "def load_gold(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gold = json.load(f)\n",
    "    return {normalize(q): set(normalize(d) for d in docs) for q, docs in gold.items()}\n",
    "\n",
    "def load_predictions_jsonl(path: Path):\n",
    "    preds = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            qid = normalize(obj[\"query_id\"])\n",
    "            ranks = [normalize(x) for x in obj[\"ranks\"]]\n",
    "            preds[qid] = ranks\n",
    "    return preds\n",
    "\n",
    "def pr_points_for_query(ranked, gold_set):\n",
    "    if not gold_set:\n",
    "        return [], []\n",
    "    tp = 0\n",
    "    precisions, recalls = [], []\n",
    "    G = len(gold_set)\n",
    "    for i, doc in enumerate(ranked, 1):\n",
    "        if doc in gold_set:\n",
    "            tp += 1\n",
    "        precisions.append(tp / i)\n",
    "        recalls.append(tp / G)\n",
    "    return precisions, recalls\n",
    "\n",
    "def interpolated_11_for_model(preds, gold):\n",
    "    R = [i/10 for i in range(11)]  # 0.0 .. 1.0\n",
    "    per_query = []\n",
    "    for qid, gold_set in gold.items():\n",
    "        ranked = preds.get(qid, [])\n",
    "        P, r = pr_points_for_query(ranked, gold_set)\n",
    "        if not P:\n",
    "            continue\n",
    "        pairs = list(zip(r, P))\n",
    "        interps = []\n",
    "        for r0 in R:\n",
    "            pmax = 0.0\n",
    "            for rq, pq in pairs:\n",
    "                if rq >= r0 and pq > pmax:\n",
    "                    pmax = pq\n",
    "            interps.append(pmax)\n",
    "        per_query.append(interps)\n",
    "    if not per_query:\n",
    "        return R, [0.0]*11, 0.0\n",
    "    mean_pts = [sum(col)/len(col) for col in zip(*per_query)]\n",
    "    auc = sum(mean_pts) / len(mean_pts)  # mean of the 11 interpolated points\n",
    "    return R, mean_pts, auc\n",
    "\n",
    "def collect_model_curves(folder: Path, gold, exclude_baselines=True):\n",
    "    curves = []\n",
    "    for fp in sorted(folder.glob(\"*.jsonl\")):\n",
    "        name = fp.name.lower()\n",
    "        if exclude_baselines and (\"me5\" in name or \"jina\" in name):\n",
    "            continue\n",
    "        preds = load_predictions_jsonl(fp)\n",
    "        _, P, _ = interpolated_11_for_model(preds, gold)\n",
    "        curves.append(P)\n",
    "    return curves\n",
    "\n",
    "def average_curves(curve_list):\n",
    "    if not curve_list:\n",
    "        return [0.0]*11\n",
    "    cols = list(zip(*curve_list))\n",
    "    return [sum(c)/len(c) for c in cols]\n",
    "\n",
    "gold = load_gold(GOLD_PATH)\n",
    "\n",
    "listwise_curves  = collect_model_curves(LISTWISE_DIR,  gold, exclude_baselines=EXCLUDE_BASELINES)\n",
    "pointwise_curves = collect_model_curves(POINTWISE_DIR, gold, exclude_baselines=EXCLUDE_BASELINES)\n",
    "\n",
    "R = [i/10 for i in range(11)]\n",
    "avg_listwise  = average_curves(listwise_curves)\n",
    "avg_pointwise = average_curves(pointwise_curves)\n",
    "\n",
    "auc_listwise  = sum(avg_listwise)  / len(avg_listwise)\n",
    "auc_pointwise = sum(avg_pointwise) / len(avg_pointwise)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(R, avg_listwise,  marker=\"o\", label=f\"Listwise (AUC {auc_listwise:.2f})\")\n",
    "plt.plot(R, avg_pointwise, marker=\"o\", label=f\"Pseudo-pointwise (AUC {auc_pointwise:.2f})\")\n",
    "plt.xlim(0,1); plt.ylim(0,1)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision (11-pt interpolated)\")\n",
    "plt.title(\"Average Precision–Recall (11-pt) across models\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "avg_png = OUT_DIR / \"pr_interpolated_average_listwise_vs_pointwise.png\"\n",
    "plt.savefig(avg_png, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "avg_csv = OUT_DIR / \"pr_interpolated_average_listwise_vs_pointwise.csv\"\n",
    "with open(avg_csv, \"w\", encoding=\"utf-8\") as f:\n",
    "    header = [\"scenario\"] + [f\"r={r:.1f}\" for r in R] + [\"AUC_11pt\"]\n",
    "    f.write(\",\".join(header) + \"\\n\")\n",
    "    f.write(\",\".join([\"Listwise\"]        + [f\"{p:.4f}\" for p in avg_listwise]  + [f\"{auc_listwise:.4f}\"])  + \"\\n\")\n",
    "    f.write(\",\".join([\"Pseudo-pointwise\"]+ [f\"{p:.4f}\" for p in avg_pointwise] + [f\"{auc_pointwise:.4f}\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fe60b",
   "metadata": {},
   "source": [
    "## Extra analysis experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a94da6",
   "metadata": {},
   "source": [
    "## Scatterplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "auc_data = []\n",
    "for model, scen_curves in by_model.items():\n",
    "    if \"Listwise\" in scen_curves and \"Pseudo-pointwise\" in scen_curves:\n",
    "        listwise_auc = scen_curves[\"Listwise\"][2]\n",
    "        pseudo_auc = scen_curves[\"Pseudo-pointwise\"][2]\n",
    "        auc_data.append((model, listwise_auc, pseudo_auc))\n",
    "\n",
    "df_auc = pd.DataFrame(auc_data, columns=[\"Model\", \"Listwise_AUC\", \"Pseudo_AUC\"])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_auc,\n",
    "    x=\"Listwise_AUC\", y=\"Pseudo_AUC\",\n",
    "    hue=\"Model\", s=80, palette=\"tab10\"\n",
    ")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "for _, row in df_auc.iterrows():\n",
    "    plt.text(\n",
    "        row[\"Listwise_AUC\"] + 0.005, row[\"Pseudo_AUC\"] + 0.005,\n",
    "        row[\"Model\"], fontsize=8\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Listwise AUC (11-pt Interpolated PR)\")\n",
    "plt.ylabel(\"Pseudo-pointwise AUC (11-pt Interpolated PR)\")\n",
    "plt.title(\"Listwise vs. Pseudo-pointwise Ranking — AUC Comparison\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"rankings/ranking_comparison_scatterplot.png\", dpi=300)\n",
    "plt.close()\n",
    "print(\"Scatter plot saved as ranking_comparison_scatterplot.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
