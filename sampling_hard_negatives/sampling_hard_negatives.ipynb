{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec45baa1",
   "metadata": {},
   "source": [
    "## Building the hard negatives\n",
    "The following scripts get 100 BM25 ranks and look for missing articles in the ranks. If relevant articles are missing from the 100 ranks for each query,\n",
    "then it drops lower ranks and injects relevant article ids into the lists. \n",
    "--> All the hard negatives have exactly 100 articles including the highest Bm25 ranks and relevant articles from the gold data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b46afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22417 documents.\n",
      "Loaded 203 queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 203/203 [00:03<00:00, 66.30it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    ">>> Script for extracting 100 first ranks with BM25. Results are saved in folder ranks/.\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# -------- CONFIG --------\n",
    "LANG = \"nl\"  # or \"nl\"\n",
    "TOP_K = 100\n",
    "QUERY_PATH = f\"../baselines/preprocessed_data/queries_{LANG}_clean.csv\"\n",
    "CORPUS_PATH = f\"../baselines/preprocessed_data/corpus_{LANG}_clean.csv\"\n",
    "OUTPUT_SIMPLE = f\"ranks/bm25_top{TOP_K}_ranked_results_{LANG}.json\"\n",
    "OUTPUT_DETAILED = f\"ranks/bm25_top{TOP_K}_ranked_results_{LANG}_with_scores.jsonl\"\n",
    "# ------------------------\n",
    "\n",
    "os.makedirs(\"ranks\", exist_ok=True)\n",
    "\n",
    "# Load corpus\n",
    "df_corpus = pd.read_csv(CORPUS_PATH)\n",
    "corpus_texts = df_corpus[\"article\"].astype(str).tolist()\n",
    "corpus_ids = df_corpus[\"id\"].astype(str).tolist()\n",
    "tokenized_corpus = [doc.split() for doc in corpus_texts]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus, k1=1.0, b=0.6)\n",
    "\n",
    "print(f\"Loaded {len(corpus_ids)} documents.\")\n",
    "\n",
    "# Load queries\n",
    "df_queries = pd.read_csv(QUERY_PATH)\n",
    "queries = df_queries[[\"id\", \"question\", \"article_ids\"]].astype(str).values.tolist()\n",
    "\n",
    "print(f\"Loaded {len(queries)} queries.\")\n",
    "\n",
    "ranked_results_simple = {}\n",
    "ranked_results_detailed = []\n",
    "\n",
    "for qid, question, relevant_str in tqdm(queries, desc=\"Processing queries\"):\n",
    "    query_tokens = question.split()  # already preprocessed\n",
    "\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    ranked_indices = scores.argsort()[::-1][:TOP_K]\n",
    "\n",
    "    ranked_doc_ids = [corpus_ids[i] for i in ranked_indices]\n",
    "    ranked_results_simple[qid] = ranked_doc_ids\n",
    "\n",
    "    ranked_list = [\n",
    "        {\n",
    "            \"doc_id\": corpus_ids[i],\n",
    "            \"score\": float(scores[i]),\n",
    "            \"rank\": rank + 1\n",
    "        }\n",
    "        for rank, i in enumerate(ranked_indices)\n",
    "    ]\n",
    "\n",
    "    ranked_results_detailed.append({\n",
    "        \"query_id\": qid,\n",
    "        \"relevant_ids\": [x.strip() for x in relevant_str.split(\",\")],\n",
    "        \"bm25_ranked_list\": ranked_list\n",
    "    })\n",
    "\n",
    "# Write output\n",
    "with open(OUTPUT_SIMPLE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ranked_results_simple, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(OUTPUT_DETAILED, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in ranked_results_detailed:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b344775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Injecting gold for FR: 100%|██████████| 203/203 [00:00<00:00, 28343.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Output written to: hard_negatives/hard_negatives_fr.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    ">>> Script for injecting relevant articles into the hard negatives. The script checks for missing relevant ids in the 100 ranks,\n",
    "for any missing article, it drops the last rank, and injects the missing id. In the end, it shuffles the lists.\n",
    "Results are saved in the folder hard_negatives/.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- CONFIG --------\n",
    "LANG = \"fr\"  # or \"fr\"\n",
    "TOP_K = 100\n",
    "BM25_PATH = f\"ranks/bm25_top{TOP_K}_ranked_results_{LANG}.json\"\n",
    "GOLD_PATH = f\"gold_standard_{LANG}.json\"\n",
    "OUTPUT_PATH = f\"hard_negatives/hard_negatives_{LANG}.jsonl\"\n",
    "# ------------------------\n",
    "\n",
    "os.makedirs(\"hard_negatives\", exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "with open(BM25_PATH, encoding=\"utf-8\") as f:\n",
    "    bm25_ranks = json.load(f)\n",
    "\n",
    "with open(GOLD_PATH, encoding=\"utf-8\") as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for qid in tqdm(sorted(bm25_ranks.keys(), key=int), desc=f\"Injecting gold for {LANG.upper()}\"):\n",
    "        top100 = bm25_ranks[qid][:]\n",
    "        gold_ids = gold_data.get(qid, [])\n",
    "\n",
    "        missing = [doc_id for doc_id in gold_ids if doc_id not in top100]\n",
    "\n",
    "        if missing:\n",
    "            to_drop = []\n",
    "            i = len(top100) - 1\n",
    "            while len(to_drop) < len(missing) and i >= 0:\n",
    "                doc_id = top100[i]\n",
    "                if doc_id not in gold_ids:\n",
    "                    to_drop.append(i)\n",
    "                i -= 1\n",
    "            for index in sorted(to_drop, reverse=True):\n",
    "                del top100[index]\n",
    "            top100 += missing\n",
    "\n",
    "        assert len(top100) == TOP_K, f\"Query {qid}: {len(top100)} docs (expected {TOP_K})\"\n",
    "        assert all(doc in top100 for doc in gold_ids), f\"Query {qid}: missing relevant doc(s)\"\n",
    "\n",
    "        random.shuffle(top100)\n",
    "\n",
    "        out_f.write(json.dumps({\n",
    "            \"query_id\": qid,\n",
    "            \"candidate_docs\": top100,\n",
    "            \"relevant_ids\": gold_ids\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Done. Output written to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b036d",
   "metadata": {},
   "source": [
    "## Verification\n",
    "This script verifies the hard negatives, if all the relevant articles are in fact included, and if the lists of 100 hard negatives include 100 unique articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e0134af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICATION REPORT ===\n",
      "Total queries checked: 203\n",
      "\n",
      "All queries contain their gold relevant articles.\n",
      "\n",
      "All queries have exactly 100 unique candidate documents.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# -------- CONFIG --------\n",
    "LANG = \"fr\"  # or \"fr\"\n",
    "INPUT_PATH = f\"hard_negatives/hard_negatives_{LANG}.jsonl\"\n",
    "GOLD_PATH = f\"gold_standard_{LANG}.json\"\n",
    "# ------------------------\n",
    "\n",
    "# Load gold standard data\n",
    "with open(GOLD_PATH, encoding=\"utf-8\") as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "# Run verification\n",
    "missing_gold = {}\n",
    "non_unique = []\n",
    "\n",
    "with open(INPUT_PATH, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        qid = entry[\"query_id\"]\n",
    "        candidates = entry[\"candidate_docs\"]\n",
    "        gold_ids = gold_data.get(qid, [])\n",
    "\n",
    "        # Check all gold IDs are present\n",
    "        missing = [doc for doc in gold_ids if doc not in candidates]\n",
    "        if missing:\n",
    "            missing_gold[qid] = missing\n",
    "\n",
    "        # Check for duplicates\n",
    "        if len(set(candidates)) != 100:\n",
    "            non_unique.append(qid)\n",
    "\n",
    "# Print results\n",
    "print(\"=== VERIFICATION REPORT ===\")\n",
    "print(f\"Total queries checked: {len(gold_data)}\")\n",
    "\n",
    "if missing_gold:\n",
    "    print(f\"\\nQueries with missing gold articles: {len(missing_gold)}\")\n",
    "    for qid, docs in missing_gold.items():\n",
    "        print(f\"Query {qid}: missing {len(docs)} → {docs}\")\n",
    "else:\n",
    "    print(\"\\nAll queries contain their gold relevant articles.\")\n",
    "\n",
    "if non_unique:\n",
    "    print(f\"\\nQueries with duplicate or incorrect number of candidates: {len(non_unique)}\")\n",
    "    print(\"Query IDs:\", non_unique)\n",
    "else:\n",
    "    print(\"\\nAll queries have exactly 100 unique candidate documents.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
