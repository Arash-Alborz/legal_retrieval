# LLM Legal Document Retrieval

This repository contains the code, evaluation scripts, and results for the thesis project on **Legal Document Retrieval using Large Language Models (LLMs)**.  
The project evaluates multiple retrieval and ranking strategies across different models (Gemini, GPT, LLaMA, Qwen, Jina, mE5, etc.) on Belgian legal texts.

---

## ğŸ“‚ Project Structure

llm_legal_document_retrieval/
â”‚
â”œâ”€â”€ baselines/                  # baseline models and results (BM25, TF-IDF, etc.)
â”‚
â”œâ”€â”€ data_processing/             # preprocessing scripts and cleaned datasets
â”‚
â”œâ”€â”€ hard_negatives_stats/        # statistics and analysis of hard negatives
â”‚
â”œâ”€â”€ llm_retrieval/               # main retrieval scripts with LLMs
â”‚
â”œâ”€â”€ ranking/                     # reranking with embeddings (Jina, mE5, etc.)
â”‚
â”œâ”€â”€ results/                     # final evaluation results and CSV summaries
â”‚   â””â”€â”€ plots/                   # evaluation plots and figures
â”‚
â”œâ”€â”€ retrievals/                  # raw retrieval outputs (per model & scenario)
â”‚
â”œâ”€â”€ sampling_hard_negatives/     # generated hard negatives for evaluation
â”‚
â”œâ”€â”€ .gitignore                   # ignored files (large embeddings, datasets, etc.)
â”œâ”€â”€ environment.yml              # conda environment definition
â”œâ”€â”€ extra_codes.ipynb            # additional experimental notebooks
â””â”€â”€ requirements.txt             # Python dependencies

---

## âš™ï¸ Setup

Clone the repository and set up the environment:

```bash
git clone https://github.com/Arash-Alborz/legal_retrieval.git
cd legal_retrieval

# Option 1: create environment via conda
conda env create -f environment.yml
conda activate llm_legal_document_retrieval

# Option 2: install requirements directly
pip install -r requirements.txt

ğŸš€ Usage

Retrieval & Ranking

Scripts for different scenarios are in /scripts/:
	â€¢	001_example_id_retrieval_v01.py â€“ ID-based retrieval
	â€¢	002_example_rc_retrieval_v01.py â€“ Relevance classification retrieval
	â€¢	003_example_pp_ranking_v01.py â€“ Pointwise ranking
	â€¢	004_example_lw_ranking_v01.py â€“ Listwise ranking
	â€¢	005_prompt_builder_all_scenarios_v01.py â€“ unified prompt builder
	â€¢	006_jina_reranking_v01.py â€“ reranking with Jina embeddings
	â€¢	007_me5_reranking_v01.py â€“ reranking with mE5 embeddings

Evaluation

Evaluation scripts are in /eval/:
	â€¢	001_retrieval_evaluation_v01.py â€“ evaluates retrieval (Precision, Recall, F1)
	â€¢	002_ranking_evaluation_v01.py â€“ evaluates ranking (R@k, MAP, MRR, nDCG)

ğŸ”— Reference

GitHub: https://github.com/Arash-Alborz/legal_retrieval