{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0f7e6f",
   "metadata": {},
   "source": [
    "## Dutch article cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def clean_article_start(text):\n",
    "    # removing tokens in the start of the  article\n",
    "    text = re.sub(r\"^(Art\\.?|Artikel|ANNEXE|DROIT FUTUR|Antérieurement|Voir note sous TITRE|BIJLAGE|Inbreuk op artikel|Voorheen)\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # removing numbers, dots, non-words\n",
    "    text = re.sub(r\"^\\d+[^\\w]*\\s*\", \"\", text)\n",
    "\n",
    "    # list of common words in the beginning\n",
    "    forbidden_words = [\"Art\", \"Artikel\", \"ANNEXE\", \"DROIT FUTUR\", \"Antérieurement\", \"Voir note sous TITRE\", \"BIJLAGE\", \"Inbreuk op artikel\", \"Voorheen\"]\n",
    "\n",
    "    # scanning for capital letter\n",
    "    for match in re.finditer(r\"[A-Z]\", text):\n",
    "        start_index = match.start()\n",
    "\n",
    "        # excluding capital letter if inside a common word\n",
    "        window_start = max(0, start_index - 10)\n",
    "        window_text = text[window_start:start_index + 10]\n",
    "\n",
    "        if any(forbidden.lower() in window_text.lower() for forbidden in forbidden_words):\n",
    "            continue\n",
    "\n",
    "        # checking next two characters after capital letter\n",
    "        next_chars = text[start_index+1:start_index+3]\n",
    "        if not re.match(r\"[\\s*'a-zA-Z]{1,2}\", next_chars):\n",
    "            continue\n",
    "\n",
    "        return text[start_index:].strip()\n",
    "\n",
    "    # If nothing found → return original text\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "with open(\"long_article_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    long_article_ids = json.load(f)\n",
    "\n",
    "df_corpus_nl = df_corpus_nl[~df_corpus_nl[\"id\"].isin(long_article_ids)]\n",
    "\n",
    "# apply cleaning\n",
    "df_corpus_nl[\"article_cleaned\"] = df_corpus_nl[\"article\"].apply(clean_article_start)\n",
    "\n",
    "# making two directories for cleaned corpus and mixed corpus for comparison\n",
    "os.makedirs(\"data/original_cleaned_mix_corpus\", exist_ok=True)\n",
    "os.makedirs(\"data/cleaned_corpus\", exist_ok=True)\n",
    "\n",
    "df_corpus_nl.to_csv(\"data/original_cleaned_mix_corpus/original_cleaned_mix_nl_corpus.csv\", index=False)\n",
    "\n",
    "\n",
    "df_corpus_nl_original_format = df_corpus_nl[[\"id\", \"reference\", \"article_cleaned\"]].rename(columns={\"article_cleaned\": \"article\"})\n",
    "df_corpus_nl_original_format.to_csv(\"data/cleaned_corpus/corpus_nl_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c84d1",
   "metadata": {},
   "source": [
    "## French article cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def clean_article_start(text):\n",
    "    # removing tokens in the start of the  article\n",
    "    text = re.sub(r\"[\\(\\[]\\s*(ancien article|ancien art|erronément intitulé art\\.?)\\s*\\d+[^\\]\\)]*[\\)\\]]\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"^(Art\\.?|Article|ANNEXE|DROIT FUTUR|Antérieurement|Voir note sous TITRE|ancien article|Infraction à l'article)\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # removing numbers, dots, non-words\n",
    "    text = re.sub(r\"^\\d+[^\\w]*\\s*\", \"\", text)\n",
    "\n",
    "    # list of common words in the beginning\n",
    "    forbidden_words = [\"Art\", \"Article\", \"ANNEXE\", \"DROIT FUTUR\", \"Antérieurement\", \"Voir note sous TITRE\", \"ancien article\", \"Infraction à l'article\"]\n",
    "\n",
    "    # scanning for capital letter\n",
    "    for match in re.finditer(r\"[A-Z]\", text):\n",
    "        start_index = match.start()\n",
    "\n",
    "        # excluding capital letter if inside a common word\n",
    "        window_start = max(0, start_index - 10)\n",
    "        window_text = text[window_start:start_index + 10]\n",
    "\n",
    "        if any(forbidden.lower() in window_text.lower() for forbidden in forbidden_words):\n",
    "            continue\n",
    "\n",
    "        # checking next two characters after capital letter\n",
    "        next_chars = text[start_index+1:start_index+3]\n",
    "        if not re.match(r\"[\\s*'a-zA-Z]{1,2}\", next_chars):\n",
    "            continue\n",
    "\n",
    "        return text[start_index:].strip()\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "with open(\"long_article_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    long_article_ids = json.load(f)\n",
    "\n",
    "df_corpus_fr = df_corpus_fr[~df_corpus_fr[\"id\"].isin(long_article_ids)]\n",
    "# apply cleaning\n",
    "df_corpus_fr[\"article_cleaned\"] = df_corpus_fr[\"article\"].apply(clean_article_start)\n",
    "\n",
    "# making two directories for cleaned corpus and mixed corpus for comparison\n",
    "os.makedirs(\"data/original_cleaned_mix_corpus\", exist_ok=True)\n",
    "os.makedirs(\"data/cleaned_corpus\", exist_ok=True)\n",
    "\n",
    "df_corpus_fr.to_csv(\"data/original_cleaned_mix_corpus/original_cleaned_mix_fr_corpus.csv\", index=False)\n",
    "\n",
    "df_corpus_fr_original_format = df_corpus_fr[[\"id\", \"reference\", \"article_cleaned\"]].rename(columns={\"article_cleaned\": \"article\"})\n",
    "df_corpus_fr_original_format.to_csv(\"data/cleaned_corpus/corpus_fr_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
