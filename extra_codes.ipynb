{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0f7e6f",
   "metadata": {},
   "source": [
    "## Dutch article cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def clean_article_start(text):\n",
    "    # removing tokens in the start of the  article\n",
    "    text = re.sub(r\"^(Art\\.?|Artikel|ANNEXE|DROIT FUTUR|Antérieurement|Voir note sous TITRE|BIJLAGE|Inbreuk op artikel|Voorheen)\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # removing numbers, dots, non-words\n",
    "    text = re.sub(r\"^\\d+[^\\w]*\\s*\", \"\", text)\n",
    "\n",
    "    # list of common words in the beginning\n",
    "    forbidden_words = [\"Art\", \"Artikel\", \"ANNEXE\", \"DROIT FUTUR\", \"Antérieurement\", \"Voir note sous TITRE\", \"BIJLAGE\", \"Inbreuk op artikel\", \"Voorheen\"]\n",
    "\n",
    "    # scanning for capital letter\n",
    "    for match in re.finditer(r\"[A-Z]\", text):\n",
    "        start_index = match.start()\n",
    "\n",
    "        # excluding capital letter if inside a common word\n",
    "        window_start = max(0, start_index - 10)\n",
    "        window_text = text[window_start:start_index + 10]\n",
    "\n",
    "        if any(forbidden.lower() in window_text.lower() for forbidden in forbidden_words):\n",
    "            continue\n",
    "\n",
    "        # checking next two characters after capital letter\n",
    "        next_chars = text[start_index+1:start_index+3]\n",
    "        if not re.match(r\"[\\s*'a-zA-Z]{1,2}\", next_chars):\n",
    "            continue\n",
    "\n",
    "        return text[start_index:].strip()\n",
    "\n",
    "    # If nothing found → return original text\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# ---> REMOVING LONG ARTICLES BEFORE CLEANING <--- Can take the next 3 lines of code out when working with the whole corpus\n",
    "with open(\"long_article_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    long_article_ids = json.load(f)\n",
    "\n",
    "df_corpus_nl = df_corpus_nl[~df_corpus_nl[\"id\"].isin(long_article_ids)]\n",
    "\n",
    "# apply cleaning\n",
    "df_corpus_nl[\"article_cleaned\"] = df_corpus_nl[\"article\"].apply(clean_article_start)\n",
    "\n",
    "# making two directories for cleaned corpus and mixed corpus for comparison\n",
    "os.makedirs(\"data/original_cleaned_mix_corpus\", exist_ok=True)\n",
    "os.makedirs(\"data/cleaned_corpus\", exist_ok=True)\n",
    "\n",
    "df_corpus_nl.to_csv(\"data/original_cleaned_mix_corpus/original_cleaned_mix_nl_corpus.csv\", index=False)\n",
    "\n",
    "\n",
    "df_corpus_nl_original_format = df_corpus_nl[[\"id\", \"reference\", \"article_cleaned\"]].rename(columns={\"article_cleaned\": \"article\"})\n",
    "df_corpus_nl_original_format.to_csv(\"data/cleaned_corpus/corpus_nl_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Created a mixed CSV of original and cleaned Dutch article texts for comparison.\")\n",
    "\n",
    "print(\"Saved cleaned Dutch corpus as CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c84d1",
   "metadata": {},
   "source": [
    "## French article cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def clean_article_start(text):\n",
    "    # removing tokens in the start of the  article\n",
    "    text = re.sub(r\"[\\(\\[]\\s*(ancien article|ancien art|erronément intitulé art\\.?)\\s*\\d+[^\\]\\)]*[\\)\\]]\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"^(Art\\.?|Article|ANNEXE|DROIT FUTUR|Antérieurement|Voir note sous TITRE|ancien article|Infraction à l'article)\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # removing numbers, dots, non-words\n",
    "    text = re.sub(r\"^\\d+[^\\w]*\\s*\", \"\", text)\n",
    "\n",
    "    # list of common words in the beginning\n",
    "    forbidden_words = [\"Art\", \"Article\", \"ANNEXE\", \"DROIT FUTUR\", \"Antérieurement\", \"Voir note sous TITRE\", \"ancien article\", \"Infraction à l'article\"]\n",
    "\n",
    "    # scanning for capital letter\n",
    "    for match in re.finditer(r\"[A-Z]\", text):\n",
    "        start_index = match.start()\n",
    "\n",
    "        # excluding capital letter if inside a common word\n",
    "        window_start = max(0, start_index - 10)\n",
    "        window_text = text[window_start:start_index + 10]\n",
    "\n",
    "        if any(forbidden.lower() in window_text.lower() for forbidden in forbidden_words):\n",
    "            continue\n",
    "\n",
    "        # checking next two characters after capital letter\n",
    "        next_chars = text[start_index+1:start_index+3]\n",
    "        if not re.match(r\"[\\s*'a-zA-Z]{1,2}\", next_chars):\n",
    "            continue\n",
    "\n",
    "        return text[start_index:].strip()\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# ---> REMOVING LONG ARTICLES BEFORE CLEANING <--- Can take the next 3 lines of code out when working with the whole corpus\n",
    "with open(\"long_article_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    long_article_ids = json.load(f)\n",
    "\n",
    "df_corpus_fr = df_corpus_fr[~df_corpus_fr[\"id\"].isin(long_article_ids)]\n",
    "# apply cleaning\n",
    "df_corpus_fr[\"article_cleaned\"] = df_corpus_fr[\"article\"].apply(clean_article_start)\n",
    "\n",
    "# making two directories for cleaned corpus and mixed corpus for comparison\n",
    "os.makedirs(\"data/original_cleaned_mix_corpus\", exist_ok=True)\n",
    "os.makedirs(\"data/cleaned_corpus\", exist_ok=True)\n",
    "\n",
    "df_corpus_fr.to_csv(\"data/original_cleaned_mix_corpus/original_cleaned_mix_fr_corpus.csv\", index=False)\n",
    "\n",
    "df_corpus_fr_original_format = df_corpus_fr[[\"id\", \"reference\", \"article_cleaned\"]].rename(columns={\"article_cleaned\": \"article\"})\n",
    "df_corpus_fr_original_format.to_csv(\"data/cleaned_corpus/corpus_fr_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Created a mixed CSV of original and cleaned French article texts for comparison.\")\n",
    "print(\"Saved cleaned French corpus as CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2414f13",
   "metadata": {},
   "source": [
    "## SAMPLING CODES (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82922ec8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bm25_hard_negatives_fr.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m jsonl_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbm25_hard_negatives_fr.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read and print contents\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjsonl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m         item \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_legal_document_retrieval/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bm25_hard_negatives_fr.jsonl'"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "corpus = load_from_disk(\"/content/data/cleaned/corpus\")\n",
    "test = load_from_disk(\"/content/data/cleaned/test\")\n",
    "\n",
    "corpus_fr = corpus['fr']\n",
    "test_fr = test['fr']\n",
    "\n",
    "\n",
    "### WITH SCORES AND RANKS FOR FRENCH\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# Load cleaned corpus and test queries (French as example)\n",
    "corpus = load_from_disk(\"data/cleaned/corpus\")['fr']\n",
    "test_queries = load_from_disk(\"data/cleaned/test\")['fr']\n",
    "\n",
    "# Prepare corpus documents and their IDs\n",
    "corpus_docs = [doc['article'] for doc in corpus]\n",
    "corpus_ids = [str(doc['id']) for doc in corpus]\n",
    "\n",
    "# Tokenize corpus with ToktokTokenizer\n",
    "tokenized_corpus = [tokenizer.tokenize(doc.lower()) for doc in corpus_docs]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Prepare output\n",
    "output = []\n",
    "\n",
    "for query in tqdm(test_queries):\n",
    "\n",
    "    query_id = query['id']\n",
    "    query_text = query['question']\n",
    "    \n",
    "    # Get relevant article IDs for this query\n",
    "    relevant_ids = [id_.strip() for id_ in query['article_ids'].split(\",\") if id_.strip() != \"\"]\n",
    "    num_relevant = len(relevant_ids)\n",
    "    \n",
    "    # Calculate how many negatives needed\n",
    "    num_negatives_needed = 100 - num_relevant\n",
    "    \n",
    "    # Tokenize query text using ToktokTokenizer\n",
    "    tokenized_query = tokenizer.tokenize(query_text.lower())\n",
    "\n",
    "    # BM25 scoring\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Combine document ID, score and index\n",
    "    scored_docs = [\n",
    "        {\"doc_id\": corpus_ids[idx], \"score\": float(bm25_scores[idx]), \"rank\": None}\n",
    "        for idx in range(len(bm25_scores))\n",
    "    ]\n",
    "    \n",
    "    # Sort by score (high to low) → rank them properly\n",
    "    scored_docs = sorted(scored_docs, key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    # Add final rank after sorting (rank 0 = highest score)\n",
    "    for final_rank, doc in enumerate(scored_docs):\n",
    "        doc[\"rank\"] = final_rank + 1  # make ranks 1-based\n",
    "\n",
    "    # Select negatives (skip relevant ids)\n",
    "    hard_negatives = []\n",
    "    \n",
    "    for doc in scored_docs:\n",
    "        if doc[\"doc_id\"] not in relevant_ids:\n",
    "            hard_negatives.append(doc)\n",
    "        if len(hard_negatives) >= num_negatives_needed:\n",
    "            break\n",
    "\n",
    "    # Save full ranked list + hard negatives\n",
    "    output.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"query_text\": query_text,\n",
    "        \"relevant_ids\": relevant_ids,\n",
    "        \"bm25_ranked_list\": scored_docs,  # FULL ranked list with doc_id, score, rank\n",
    "        \"hard_negatives\": hard_negatives  # selected hard negatives only\n",
    "    })\n",
    "\n",
    "# Save to JSONL\n",
    "os.makedirs(\"data/bm25_sampling\", exist_ok=True)\n",
    "output_path = \"data/bm25_sampling/bm25_with_scores_and_ranks_fr.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in output:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "#print(\"✅ BM25 sampling complete and saved (with scores and ranks).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c527e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WITH SCORES AND RANKS FOR DUTCH\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# Load cleaned corpus and test queries (Dutch)\n",
    "corpus = load_from_disk(\"data/cleaned/corpus\")['nl']\n",
    "test_queries = load_from_disk(\"data/cleaned/test\")['nl']\n",
    "\n",
    "# Prepare corpus documents and their IDs\n",
    "corpus_docs = [doc['article'] for doc in corpus]\n",
    "corpus_ids = [str(doc['id']) for doc in corpus]\n",
    "\n",
    "# Tokenize corpus with ToktokTokenizer\n",
    "tokenized_corpus = [tokenizer.tokenize(doc.lower()) for doc in corpus_docs]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Prepare output\n",
    "output = []\n",
    "\n",
    "for query in tqdm(test_queries):\n",
    "\n",
    "    query_id = query['id']\n",
    "    query_text = query['question']\n",
    "    \n",
    "    # Get relevant article IDs for this query\n",
    "    relevant_ids = [id_.strip() for id_ in query['article_ids'].split(\",\") if id_.strip() != \"\"]\n",
    "    num_relevant = len(relevant_ids)\n",
    "    \n",
    "    # Calculate how many negatives needed\n",
    "    num_negatives_needed = 100 - num_relevant\n",
    "    \n",
    "    # Tokenize query text using ToktokTokenizer\n",
    "    tokenized_query = tokenizer.tokenize(query_text.lower())\n",
    "\n",
    "    # BM25 scoring\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Combine document ID, score and index\n",
    "    scored_docs = [\n",
    "        {\"doc_id\": corpus_ids[idx], \"score\": float(bm25_scores[idx]), \"rank\": None}\n",
    "        for idx in range(len(bm25_scores))\n",
    "    ]\n",
    "    \n",
    "    # Sort by score (high to low) → rank them properly\n",
    "    scored_docs = sorted(scored_docs, key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    # Add final rank after sorting (rank 0 = highest score)\n",
    "    for final_rank, doc in enumerate(scored_docs):\n",
    "        doc[\"rank\"] = final_rank + 1  # make ranks 1-based\n",
    "\n",
    "    # Select negatives (skip relevant ids)\n",
    "    hard_negatives = []\n",
    "    \n",
    "    for doc in scored_docs:\n",
    "        if doc[\"doc_id\"] not in relevant_ids:\n",
    "            hard_negatives.append(doc)\n",
    "        if len(hard_negatives) >= num_negatives_needed:\n",
    "            break\n",
    "\n",
    "    # Save full ranked list + hard negatives\n",
    "    output.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"query_text\": query_text,\n",
    "        \"relevant_ids\": relevant_ids,\n",
    "        \"bm25_ranked_list\": scored_docs,  # FULL ranked list with doc_id, score, rank\n",
    "        \"hard_negatives\": hard_negatives  # selected hard negatives only\n",
    "    })\n",
    "\n",
    "# Save to JSONL\n",
    "os.makedirs(\"data/bm25_sampling\", exist_ok=True)\n",
    "output_path = \"data/bm25_sampling/bm25_with_scores_and_ranks_nl.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in output:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"BM25 sampling complete and saved (with scores and ranks) for DUTCH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a687fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR LOADING AND CHECKING\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Load French BM25 results\n",
    "fr_path = \"data/bm25_sampling/bm25_with_scores_and_ranks_fr.jsonl\"\n",
    "with open(fr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    french_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"✅ Loaded {len(french_data)} French BM25 entries\")\n",
    "print(\"Example:\")\n",
    "print(french_data[:5])  # Show first entry\n",
    "\n",
    "# Load Dutch BM25 results\n",
    "nl_path = \"data/bm25_sampling/bm25_hard_negatives_nl.jsonl\"\n",
    "with open(nl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dutch_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"\\n✅ Loaded {len(dutch_data)} Dutch BM25 entries\")\n",
    "print(\"Example:\")\n",
    "print(dutch_data[5])  # Show first entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0504169",
   "metadata": {},
   "source": [
    "## The following code is for removing all the relevant article ids from the data. It can be used but I will not use it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning and saving as CSV and dataset.arrows\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "ds_test = load_dataset(\"clips/bBSARD\", \"test\")\n",
    "test_fr = ds_test[\"fr\"]\n",
    "test_nl = ds_test[\"nl\"]\n",
    "\n",
    "# Load long article citation map\n",
    "with open(\"queries_citing_long_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    queries_citing_long_articles = json.load(f)\n",
    "\n",
    "long_articles_fr = queries_citing_long_articles[\"fr\"]\n",
    "long_articles_nl = queries_citing_long_articles[\"nl\"]\n",
    "\n",
    "# Cleaning function\n",
    "def clean_query_set(test_set, long_article_map, lang_label):\n",
    "    cleaned = []\n",
    "    for query in test_set:\n",
    "        query_id = str(query[\"id\"])\n",
    "        relevant_ids = [id_.strip() for id_ in query[\"article_ids\"].split(\",\")]\n",
    "\n",
    "        if len(relevant_ids) > 10:\n",
    "            continue\n",
    "\n",
    "        if query_id in long_article_map:\n",
    "            long_ids = set(long_article_map[query_id])\n",
    "            relevant_ids = [id_ for id_ in relevant_ids if id_ not in long_ids]\n",
    "\n",
    "        if len(relevant_ids) == 0:\n",
    "            continue\n",
    "\n",
    "        query_cleaned = dict(query)\n",
    "        query_cleaned[\"article_ids\"] = \", \".join(relevant_ids)\n",
    "        cleaned.append(query_cleaned)\n",
    "    print(f\"Cleaned {len(cleaned)} {lang_label} queries.\")\n",
    "    return cleaned\n",
    "\n",
    "cleaned_fr = clean_query_set(test_fr, long_articles_fr, \"French\")\n",
    "cleaned_nl = clean_query_set(test_nl, long_articles_nl, \"Dutch\")\n",
    "\n",
    "os.makedirs(\"data/cleaned_queries_csv\", exist_ok=True)\n",
    "pd.DataFrame(cleaned_fr).to_csv(\"data/cleaned_queries_csv/cleaned_test_queries_fr.csv\", index=False)\n",
    "pd.DataFrame(cleaned_nl).to_csv(\"data/cleaned_queries_csv/cleaned_test_queries_nl.csv\", index=False)\n",
    "\n",
    "ds_cleaned_fr = Dataset.from_list(cleaned_fr)\n",
    "ds_cleaned_nl = Dataset.from_list(cleaned_nl)\n",
    "\n",
    "ds_cleaned = DatasetDict({\n",
    "    \"fr\": ds_cleaned_fr,\n",
    "    \"nl\": ds_cleaned_nl\n",
    "})\n",
    "os.makedirs(\"data/cleaned_queries_ds\", exist_ok=True)\n",
    "ds_cleaned.save_to_disk(\"data/cleaned_queries_ds/cleaned_test_queries\")\n",
    "print(\"Saved cleaned test queries to HuggingFace dataset format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fe71c",
   "metadata": {},
   "source": [
    "## counting words of hard negatives for estimation on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27bb831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing language: FR\n",
      "Loading original corpus for language: fr\n",
      "Loaded 22417 documents from CSV.\n",
      "Created ID-to-text mapping.\n",
      "Loading hard negatives for language: fr\n",
      "Loaded 203 queries with hard negatives.\n",
      "\n",
      "Per-query total word counts (first 5 rows):\n",
      "  query_id  total_words  missing_docs\n",
      "0        4        22801             0\n",
      "1        7        24975             0\n",
      "2       16        13061             0\n",
      "3       17        18037             0\n",
      "4       25        14777             0\n",
      "Saved results to: hard_negatives_stats/query_word_counts_fr.csv\n",
      "\n",
      "Processing language: NL\n",
      "Loading original corpus for language: nl\n",
      "Loaded 22417 documents from CSV.\n",
      "Created ID-to-text mapping.\n",
      "Loading hard negatives for language: nl\n",
      "Loaded 203 queries with hard negatives.\n",
      "\n",
      "Per-query total word counts (first 5 rows):\n",
      "  query_id  total_words  missing_docs\n",
      "0        4        10448             0\n",
      "1        7        18852             0\n",
      "2       16        15842             0\n",
      "3       17        17496             0\n",
      "4       25        20045             0\n",
      "Saved results to: hard_negatives_stats/query_word_counts_nl.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIG: paths — do NOT change\n",
    "corpus_paths = {\n",
    "    \"fr\": Path(\"data_processing/data/original_csv/corpus_fr.csv\"),\n",
    "    \"nl\": Path(\"data_processing/data/original_csv/corpus_nl.csv\")\n",
    "}\n",
    "hard_negatives_paths = {\n",
    "    \"fr\": Path(\"sampling_hard_negatives/hard_negatives/hard_negatives_fr.jsonl\"),\n",
    "    \"nl\": Path(\"sampling_hard_negatives/hard_negatives/hard_negatives_nl.jsonl\")\n",
    "}\n",
    "\n",
    "# Output folder\n",
    "output_dir = Path(\"hard_negatives_stats\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for lang in [\"fr\", \"nl\"]:\n",
    "    print(f\"\\nProcessing language: {lang.upper()}\")\n",
    "\n",
    "    # Load corpus CSV\n",
    "    print(f\"Loading original corpus for language: {lang}\")\n",
    "    df_corpus = pd.read_csv(corpus_paths[lang])\n",
    "    print(f\"Loaded {len(df_corpus)} documents from CSV.\")\n",
    "\n",
    "    # Create ID → article mapping\n",
    "    id_to_doc = dict(zip(df_corpus['id'].astype(str), df_corpus['article']))\n",
    "    print(\"Created ID-to-text mapping.\")\n",
    "\n",
    "    # Load hard negatives JSONL\n",
    "    print(f\"Loading hard negatives for language: {lang}\")\n",
    "    entries = []\n",
    "    with open(hard_negatives_paths[lang], encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            entries.append(json.loads(line.strip()))\n",
    "\n",
    "    print(f\"Loaded {len(entries)} queries with hard negatives.\")\n",
    "\n",
    "    # Collect per-query total word counts\n",
    "    results = []\n",
    "\n",
    "    for entry in entries:\n",
    "        query_id = entry['query_id']\n",
    "        candidate_ids = entry['candidate_docs']\n",
    "\n",
    "        total_words = 0\n",
    "        missing_docs = 0\n",
    "\n",
    "        for doc_id in candidate_ids:\n",
    "            text = id_to_doc.get(doc_id, \"\").strip()\n",
    "            if not text:\n",
    "                missing_docs += 1\n",
    "                continue\n",
    "            total_words += len(text.split())\n",
    "\n",
    "        results.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"total_words\": total_words,\n",
    "            \"missing_docs\": missing_docs\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\nPer-query total word counts (first 5 rows):\")\n",
    "    print(df_results.head())\n",
    "\n",
    "    # Save to CSV\n",
    "    output_path = output_dir / f\"query_word_counts_{lang}.csv\"\n",
    "    df_results.to_csv(output_path, index=False)\n",
    "    print(f\"Saved results to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2bcea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Statistics for language: FR ===\n",
      "\n",
      "Total Words (in 100 docs per query):\n",
      "count      203.000000\n",
      "mean     23812.359606\n",
      "std       6935.895663\n",
      "min       6265.000000\n",
      "25%      18975.500000\n",
      "50%      23403.000000\n",
      "75%      28420.500000\n",
      "max      40998.000000\n",
      "Name: total_words, dtype: float64\n",
      "\n",
      "Missing Docs (in 100 docs per query):\n",
      "count    203.0\n",
      "mean       0.0\n",
      "std        0.0\n",
      "min        0.0\n",
      "25%        0.0\n",
      "50%        0.0\n",
      "75%        0.0\n",
      "max        0.0\n",
      "Name: missing_docs, dtype: float64\n",
      "\n",
      "\n",
      "=== Statistics for language: NL ===\n",
      "\n",
      "Total Words (in 100 docs per query):\n",
      "count      203.000000\n",
      "mean     22563.310345\n",
      "std       6651.436819\n",
      "min       8261.000000\n",
      "25%      17574.000000\n",
      "50%      22393.000000\n",
      "75%      26977.000000\n",
      "max      41883.000000\n",
      "Name: total_words, dtype: float64\n",
      "\n",
      "Missing Docs (in 100 docs per query):\n",
      "count    203.0\n",
      "mean       0.0\n",
      "std        0.0\n",
      "min        0.0\n",
      "25%        0.0\n",
      "50%        0.0\n",
      "75%        0.0\n",
      "max        0.0\n",
      "Name: missing_docs, dtype: float64\n",
      "\n",
      "\n",
      "Statistics saved to: hard_negatives_stats/query_word_counts_statistics.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "output_dir = Path(\"hard_negatives_stats\")\n",
    "output_txt = output_dir / \"query_word_counts_statistics.txt\"\n",
    "\n",
    "with open(output_txt, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for lang in [\"fr\", \"nl\"]:\n",
    "        input_csv = output_dir / f\"query_word_counts_{lang}.csv\"\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        f_out.write(f\"=== Statistics for language: {lang.upper()} ===\\n\")\n",
    "        print(f\"=== Statistics for language: {lang.upper()} ===\")\n",
    "\n",
    "        desc_words = df['total_words'].describe()\n",
    "        desc_missing = df['missing_docs'].describe()\n",
    "\n",
    "        f_out.write(\"\\nTotal Words (in 100 docs per query):\\n\")\n",
    "        print(\"\\nTotal Words (in 100 docs per query):\")\n",
    "        f_out.write(desc_words.to_string() + \"\\n\")\n",
    "        print(desc_words)\n",
    "\n",
    "        f_out.write(\"\\nMissing Docs (in 100 docs per query):\\n\")\n",
    "        print(\"\\nMissing Docs (in 100 docs per query):\")\n",
    "        f_out.write(desc_missing.to_string() + \"\\n\\n\")\n",
    "        print(desc_missing)\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(f\"Statistics saved to: {output_txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e28601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram saved to: hard_negatives_stats/query_word_counts_histogram_fr.png\n",
      "Histogram saved to: hard_negatives_stats/query_word_counts_histogram_nl.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output folder\n",
    "output_dir = Path(\"hard_negatives_stats\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for lang in [\"fr\", \"nl\"]:\n",
    "    input_csv = f\"query_word_counts_{lang}.csv\"\n",
    "\n",
    "    # Load per-query results\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df['total_words'], bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Distribution of Total Words Across Queries ({lang.upper()})\")\n",
    "    plt.xlabel(\"Total Words in 100 Documents\")\n",
    "    plt.ylabel(\"Number of Queries\")\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "    # Save\n",
    "    output_png = output_dir / f\"query_word_counts_histogram_{lang}.png\"\n",
    "    plt.savefig(output_png, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Histogram saved to: {output_png}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28e918b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar plot saved to: hard_negatives_stats/query_word_counts_barplot_fr.png\n",
      "Bar plot saved to: hard_negatives_stats/query_word_counts_barplot_nl.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output folder\n",
    "output_dir = Path(\"hard_negatives_stats\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for lang in [\"fr\", \"nl\"]:\n",
    "    input_csv = f\"query_word_counts_{lang}.csv\"\n",
    "\n",
    "    # Load results\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Sort by query_id\n",
    "    df['query_id'] = df['query_id'].astype(str)\n",
    "    df = df.sort_values(by='query_id')\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.bar(df['query_id'], df['total_words'], color='steelblue')\n",
    "    plt.title(f\"Total Words in 100 Candidate Docs per Query ({lang.upper()})\")\n",
    "    plt.xlabel(\"Query ID\")\n",
    "    plt.ylabel(\"Total Words\")\n",
    "    plt.xticks(rotation=90, fontsize=6)  # rotate x-axis labels for readability\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    output_png = output_dir / f\"query_word_counts_barplot_{lang}.png\"\n",
    "    plt.savefig(output_png, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Bar plot saved to: {output_png}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b279554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box plot saved to: hard_negatives_stats/query_word_counts_boxplot_fr_nl.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/rg5htfh543ddx8vy8gstpvwm0000gn/T/ipykernel_52794/2350424710.py:18: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([data[\"FR\"], data[\"NL\"]],\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output folder\n",
    "output_dir = Path(\"hard_negatives_stats\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load both datasets\n",
    "data = {}\n",
    "for lang in [\"fr\", \"nl\"]:\n",
    "    input_csv = f\"query_word_counts_{lang}.csv\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    data[lang.upper()] = df['total_words']\n",
    "\n",
    "# Plot boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([data[\"FR\"], data[\"NL\"]],\n",
    "            vert=True,\n",
    "            patch_artist=True,\n",
    "            labels=[\"FR\", \"NL\"],\n",
    "            boxprops=dict(facecolor='lightblue', color='black'),\n",
    "            medianprops=dict(color='red'))\n",
    "\n",
    "plt.title(\"Box Plot of Total Words Across Queries (FR vs NL)\")\n",
    "plt.ylabel(\"Total Words in 100 Documents\")\n",
    "plt.grid(axis='y', alpha=0.5)\n",
    "\n",
    "# Save\n",
    "output_png = output_dir / \"query_word_counts_boxplot_fr_nl.png\"\n",
    "plt.savefig(output_png, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Box plot saved to: {output_png}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ac9c3",
   "metadata": {},
   "source": [
    "## Counting tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6665f26",
   "metadata": {},
   "source": [
    "### LLAMA tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61c099fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Statistics for language: FR ===\n",
      "\n",
      "Total Tokens (in 100 docs per query):\n",
      "count      203.000000\n",
      "mean     44067.000000\n",
      "std      12753.935272\n",
      "min      11470.000000\n",
      "25%      35263.500000\n",
      "50%      43301.000000\n",
      "75%      52735.500000\n",
      "max      77976.000000\n",
      "Name: total_tokens, dtype: float64\n",
      "\n",
      "Total Words (in 100 docs per query):\n",
      "count      203.000000\n",
      "mean     23812.359606\n",
      "std       6935.895663\n",
      "min       6265.000000\n",
      "25%      18975.500000\n",
      "50%      23403.000000\n",
      "75%      28420.500000\n",
      "max      40998.000000\n",
      "Name: total_words, dtype: float64\n",
      "\n",
      "Missing Docs (in 100 docs per query):\n",
      "count    203.0\n",
      "mean       0.0\n",
      "std        0.0\n",
      "min        0.0\n",
      "25%        0.0\n",
      "50%        0.0\n",
      "75%        0.0\n",
      "max        0.0\n",
      "Name: missing_docs, dtype: float64\n",
      "\n",
      "*** TOTAL WORDS across all queries: 4833909 ***\n",
      "*** TOTAL TOKENS across all queries: 8945601 ***\n",
      "\n",
      "=== Statistics for language: NL ===\n",
      "\n",
      "Total Tokens (in 100 docs per query):\n",
      "count      203.000000\n",
      "mean     47242.541872\n",
      "std      13846.857202\n",
      "min      16840.000000\n",
      "25%      36739.000000\n",
      "50%      46843.000000\n",
      "75%      56906.500000\n",
      "max      86774.000000\n",
      "Name: total_tokens, dtype: float64\n",
      "\n",
      "Total Words (in 100 docs per query):\n",
      "count      203.000000\n",
      "mean     22563.310345\n",
      "std       6651.436819\n",
      "min       8261.000000\n",
      "25%      17574.000000\n",
      "50%      22393.000000\n",
      "75%      26977.000000\n",
      "max      41883.000000\n",
      "Name: total_words, dtype: float64\n",
      "\n",
      "Missing Docs (in 100 docs per query):\n",
      "count    203.0\n",
      "mean       0.0\n",
      "std        0.0\n",
      "min        0.0\n",
      "25%        0.0\n",
      "50%        0.0\n",
      "75%        0.0\n",
      "max        0.0\n",
      "Name: missing_docs, dtype: float64\n",
      "\n",
      "*** TOTAL WORDS across all queries: 4580352 ***\n",
      "*** TOTAL TOKENS across all queries: 9590236 ***\n",
      "\n",
      "Statistics saved to: hard_negatives_stats/token_count/query_word_counts_and_tokens_llama_statistics.txt\n"
     ]
    }
   ],
   "source": [
    "# Statistics into one text file\n",
    "stats_txt = base_dir / \"query_word_counts_and_tokens_llama_statistics.txt\"\n",
    "with open(stats_txt, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for lang in [\"fr\", \"nl\"]:\n",
    "        f_out.write(f\"=== Statistics for language: {lang.upper()} ===\\n\")\n",
    "        print(f\"\\n=== Statistics for language: {lang.upper()} ===\")\n",
    "\n",
    "        df_lang = dfs[lang]\n",
    "\n",
    "        desc_tokens = df_lang['total_tokens'].describe()\n",
    "        desc_words = df_lang['total_words'].describe()\n",
    "        desc_missing = df_lang['missing_docs'].describe()\n",
    "\n",
    "        total_tokens_all_queries = df_lang['total_tokens'].sum()\n",
    "        total_words_all_queries = df_lang['total_words'].sum()\n",
    "\n",
    "        f_out.write(\"\\nTotal Tokens (in 100 docs per query):\\n\")\n",
    "        print(\"\\nTotal Tokens (in 100 docs per query):\")\n",
    "        f_out.write(desc_tokens.to_string() + \"\\n\")\n",
    "        print(desc_tokens)\n",
    "\n",
    "        f_out.write(\"\\nTotal Words (in 100 docs per query):\\n\")\n",
    "        print(\"\\nTotal Words (in 100 docs per query):\")\n",
    "        f_out.write(desc_words.to_string() + \"\\n\")\n",
    "        print(desc_words)\n",
    "\n",
    "        f_out.write(\"\\nMissing Docs (in 100 docs per query):\\n\")\n",
    "        print(\"\\nMissing Docs (in 100 docs per query):\")\n",
    "        f_out.write(desc_missing.to_string() + \"\\n\")\n",
    "        print(desc_missing)\n",
    "\n",
    "        f_out.write(f\"\\n*** TOTAL WORDS across all queries: {total_words_all_queries} ***\\n\")\n",
    "        print(f\"\\n*** TOTAL WORDS across all queries: {total_words_all_queries} ***\")\n",
    "\n",
    "        f_out.write(f\"*** TOTAL TOKENS across all queries: {total_tokens_all_queries} ***\\n\\n\")\n",
    "        print(f\"*** TOTAL TOKENS across all queries: {total_tokens_all_queries} ***\")\n",
    "\n",
    "print(f\"\\nStatistics saved to: {stats_txt}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_legal_document_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
